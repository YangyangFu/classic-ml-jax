{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "We treat features as nodes. To build a decision tree, we need recursively/iteratively choose the best node for each level. The best node should have the most information gain after spliting.\n",
    "An ideal case would be that if there is a threshold in the feature that can 100% separate all the labels.\n",
    "\n",
    "Some pseduo code for choosing the best node for each level:\n",
    "- for each feature in features:\n",
    "  - thresholds = unique(data[feature])\n",
    "  - for each threshold in thresholds:\n",
    "    - information gain = split(data, feature, therold)\n",
    "- pick the feature that leads to the most information gain\n",
    "\n",
    "**Entropy**\n",
    "- entropy is a way to measure impurity. and ideal split would sperate the same labels in the same subtree, which leads to an entropy of 0.\n",
    "\n",
    "$$\n",
    "H = -\\sum_{i} p_i\\log p_i\n",
    "$$\n",
    "\n",
    "$p_i$ is the fraction of the `ith` label in the current dataset for spliting -> probability of each label\n",
    "\n",
    "- for regression problem, this could be represented as `MSE`\n",
    "\n",
    "**Information Gain**\n",
    "- information gain is the reduction of entropy when a split is made\n",
    "\n",
    "$$\n",
    "g = H(p^{node}) - (w^{left} H(p^{left}) + w^{right} H(p^{right}))\n",
    "$$\n",
    "\n",
    "where $H(p^{node})$ is the entropy at node, $H(p^{left})$ and $H(p^{right})$ is the entropy at the left and right subtree after spliting.\n",
    "$w^{left}$ and $w^{right}$ is are the fraction of number of samples of left and right subtrees in terms of total number of samples before splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    \"\"\"Node class for a decision tree\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, gain=None, value=None):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            feature (_type_, optional): feature represented at current node. Defaults to None.\n",
    "            threshold (_type_, optional): splitting threshold for current feature. Defaults to None.\n",
    "            left (_type_, optional): left child. Defaults to None.\n",
    "            right (_type_, optional): right child. Defaults to None.\n",
    "            gain (_type_, optional): information gain after splitting. Defaults to None.\n",
    "            value (_type_, optional): predicted value. Only available for leaf nodes. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.gain = gain\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"Decision Tree class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize an empty decision tree\n",
    "        \"\"\"\n",
    "        self.root = None\n",
    "    \n",
    "    def build_tree(self, X, y):\n",
    "        \"\"\"Build the decision tree recursively\n",
    "        \n",
    "        Args:\n",
    "            X (array-like): Input features\n",
    "            y (array-like): Target labels\n",
    "        \"\"\"\n",
    "        self.root = self._build_tree_recursive(X, y)\n",
    "    \n",
    "    def _build_tree_recursive(self, X, y):\n",
    "        \"\"\"Recursively build the decision tree\n",
    "        \n",
    "        Args:\n",
    "            X (array-like): Input features\n",
    "            y (array-like): Target labels\n",
    "        \n",
    "        Returns:\n",
    "            Node: Root node of the decision tree\n",
    "        \"\"\"\n",
    "        # Base case: if all labels are the same, create a leaf node with the predicted value\n",
    "        if len(set(y)) == 1:\n",
    "            return Node(value=y[0])\n",
    "        \n",
    "        # Find the best feature and threshold to split the data\n",
    "        best_feature, best_threshold, best_gain = self._find_best_split(X, y)\n",
    "        \n",
    "        # Create a new node with the best feature and threshold\n",
    "        node = Node(feature=best_feature, threshold=best_threshold, gain=best_gain)\n",
    "        \n",
    "        # Split the data based on the best feature and threshold\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "        X_left, y_left = X[left_indices], y[left_indices]\n",
    "        X_right, y_right = X[right_indices], y[right_indices]\n",
    "        \n",
    "        # Recursively build the left and right subtrees\n",
    "        node.left = self._build_tree_recursive(X_left, y_left)\n",
    "        node.right = self._build_tree_recursive(X_right, y_right)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def _find_best_split(self, X, y):\n",
    "        \"\"\"Find the best feature and threshold to split the data\n",
    "        \n",
    "        Args:\n",
    "            X (array-like): Input features\n",
    "            y (array-like): Target labels\n",
    "        \n",
    "        Returns:\n",
    "            int: Index of the best feature\n",
    "            float: Best threshold value\n",
    "            float: Information gain of the best split\n",
    "        \"\"\"\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_gain = -1\n",
    "        \n",
    "        # Iterate over each feature\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            \n",
    "            # Iterate over each threshold\n",
    "            for threshold in thresholds:\n",
    "                # Split the data based on the current feature and threshold\n",
    "                left_indices = X[:, feature] <= threshold\n",
    "                right_indices = X[:, feature] > threshold\n",
    "                y_left, y_right = y[left_indices], y[right_indices]\n",
    "                \n",
    "                # Calculate the information gain of the current split\n",
    "                gain = self._calculate_information_gain(y, y_left, y_right)\n",
    "                \n",
    "                # Update the best feature, threshold, and gain if the current split has higher gain\n",
    "                if gain > best_gain:\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "                    best_gain = gain\n",
    "        \n",
    "        return best_feature, best_threshold, best_gain\n",
    "    \n",
    "    def _calculate_information_gain(self, y, y_left, y_right):\n",
    "        \"\"\"Calculate the information gain of a split\n",
    "        \n",
    "        Args:\n",
    "            y (array-like): Target labels before splitting\n",
    "            y_left (array-like): Target labels in the left subtree after splitting\n",
    "            y_right (array-like): Target labels in the right subtree after splitting\n",
    "        \n",
    "        Returns:\n",
    "            float: Information gain\n",
    "        \"\"\"\n",
    "        # Calculate the entropy before splitting\n",
    "        entropy_before = self._calculate_entropy(y)\n",
    "        \n",
    "        # Calculate the entropy after splitting\n",
    "        entropy_left = self._calculate_entropy(y_left)\n",
    "        entropy_right = self._calculate_entropy(y_right)\n",
    "        \n",
    "        # Calculate the weighted average entropy after splitting\n",
    "        weight_left = len(y_left) / len(y)\n",
    "        weight_right = len(y_right) / len(y)\n",
    "        entropy_after = weight_left * entropy_left + weight_right * entropy_right\n",
    "        \n",
    "        # Calculate the information gain\n",
    "        gain = entropy_before - entropy_after\n",
    "        \n",
    "        return gain\n",
    "    \n",
    "    def _calculate_entropy(self, y):\n",
    "        \"\"\"Calculate the entropy of a set of labels\n",
    "        \n",
    "        Args:\n",
    "            y (array-like): Target labels\n",
    "        \n",
    "        Returns:\n",
    "            float: Entropy\n",
    "        \"\"\"\n",
    "        # Count the occurrences of each label\n",
    "        label_counts = np.bincount(y)\n",
    "        \n",
    "        # Calculate the probability of each label\n",
    "        probabilities = label_counts / len(y)\n",
    "        \n",
    "        # Calculate the entropy\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "        \n",
    "        return entropy\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions using the decision tree\n",
    "        \n",
    "        Args:\n",
    "            X (array-like): Input features\n",
    "        \n",
    "        Returns:\n",
    "            array-like: Predicted labels\n",
    "        \"\"\"\n",
    "        return np.array([self._predict_recursive(x, self.root) for x in X])\n",
    "    \n",
    "    def _predict_recursive(self, x, node):\n",
    "        \"\"\"Recursively make predictions using the decision tree\n",
    "        \n",
    "        Args:\n",
    "            x (array-like): Input features\n",
    "            node (Node): Current node\n",
    "        \n",
    "        Returns:\n",
    "            int: Predicted label\n",
    "        \"\"\"\n",
    "        # Base case: if the current node is a leaf node, return the predicted value\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        \n",
    "        # Recursively traverse the tree based on the feature and threshold\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._predict_recursive(x, node.left)\n",
    "        else:\n",
    "            return self._predict_recursive(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "How large should we grow the tree?\n",
    "Tree size is a tuning parameter governing the model complexity.\n",
    "- grow the tree as large as possible, say, $T_0$. Only stop the splitting process when some minimum node size (i.e., 5) is reached.\n",
    "- prune $T_0$ using `cost-complexity` pruning\n",
    "\n",
    "\n",
    "cost complexity criterion:\n",
    "\n",
    "$$\n",
    "C_{\\alpha}(T) = \\sum_{m=1}^{|T|}N_mQ_m(T) + \\alpha |T|\n",
    "$$\n",
    "\n",
    "where $T$ is a subtree we want, $|T|$ is the number of nodes in $T$, $N_m$ is the number of tree nodes in the region with node `m`.\n",
    "$Q_m(T)$ is the mean prediction errors in the subtree region. \n",
    "The idea is to find, for each $\\alpha$, the subtree $T_{\\alpha}$ to minimize $C_{\\alpha}(T)$. \n",
    "The tuning $\\alpha$ governs the tradeoff between tree size and its goodness of fit to the data. Large value will lead to small tree and vice versa. \n",
    "\n",
    "For each $\\alpha$, there will be a unique smallest subtree $T_{\\alpha}$. \n",
    "To find the $T_{\\alpha}$, we successively collapse the internal node that produces the smallest per-node increase in $\\sum_{m=1}^{|T|}N_mQ_m(T)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_complexity_pruning(self, X_val, y_val, alpha):\n",
    "    \"\"\"Perform cost complexity pruning on the decision tree\n",
    "    \n",
    "    Args:\n",
    "        X_val (array-like): Validation set features\n",
    "        y_val (array-like): Validation set labels\n",
    "        alpha (float): Complexity parameter\n",
    "        \n",
    "    Returns:\n",
    "        Node: Pruned decision tree\n",
    "    \"\"\"\n",
    "    self._calculate_subtree_cost(self.root, X_val, y_val)\n",
    "    self._prune_subtree(self.root, alpha)\n",
    "    \n",
    "    return self.root\n",
    "\n",
    "def _calculate_subtree_cost(self, node, X_val, y_val):\n",
    "    \"\"\"Recursively calculate the cost complexity criterion for each subtree\n",
    "    \n",
    "    Args:\n",
    "        node (Node): Current node\n",
    "        X_val (array-like): Validation set features\n",
    "        y_val (array-like): Validation set labels\n",
    "    \"\"\"\n",
    "    # Base case: if the current node is a leaf node, calculate the cost complexity criterion\n",
    "    if node.left is None and node.right is None:\n",
    "        node.cost = self._calculate_cost(node, X_val, y_val)\n",
    "        return\n",
    "    \n",
    "    # Recursively calculate the cost complexity criterion for the left and right subtrees\n",
    "    self._calculate_subtree_cost(node.left, X_val, y_val)\n",
    "    self._calculate_subtree_cost(node.right, X_val, y_val)\n",
    "    \n",
    "    # Calculate the cost complexity criterion for the current node\n",
    "    node.cost = node.left.cost + node.right.cost\n",
    "\n",
    "def _calculate_cost(self, node, X_val, y_val):\n",
    "    \"\"\"Calculate the cost complexity criterion for a leaf node\n",
    "    \n",
    "    Args:\n",
    "        node (Node): Leaf node\n",
    "        X_val (array-like): Validation set features\n",
    "        y_val (array-like): Validation set labels\n",
    "    \n",
    "    Returns:\n",
    "        float: Cost complexity criterion\n",
    "    \"\"\"\n",
    "    # Make predictions using the current subtree\n",
    "    y_pred = self.predict(X_val)\n",
    "    \n",
    "    # Calculate the number of misclassifications\n",
    "    misclassifications = np.sum(y_pred != y_val)\n",
    "    \n",
    "    # Calculate the cost complexity criterion\n",
    "    cost = (misclassifications / len(y_val)) + (node.gain * node.value)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def _prune_subtree(self, node, alpha):\n",
    "    \"\"\"Recursively prune the decision tree based on the complexity parameter\n",
    "    \n",
    "    Args:\n",
    "        node (Node): Current node\n",
    "        alpha (float): Complexity parameter\n",
    "    \"\"\"\n",
    "    # Base case: if the current node is a leaf node, return\n",
    "    if node.left is None and node.right is None:\n",
    "        return\n",
    "    \n",
    "    # Recursively prune the left and right subtrees\n",
    "    self._prune_subtree(node.left, alpha)\n",
    "    self._prune_subtree(node.right, alpha)\n",
    "    \n",
    "    # Check if the current node can be pruned\n",
    "    if node.cost <= alpha:\n",
    "        node.left = None\n",
    "        node.right = None\n",
    "    \n",
    "DecisionTree.cost_complexity_pruning = cost_complexity_pruning\n",
    "DecisionTree._calculate_subtree_cost = _calculate_subtree_cost\n",
    "DecisionTree._calculate_cost = _calculate_cost\n",
    "DecisionTree._prune_subtree = _prune_subtree\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-0.4.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
