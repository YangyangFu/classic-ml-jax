{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax.scipy import stats\n",
    "from random import seed, randrange"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm: Classification and Regression Tree (CART)\n",
    "Input: Training dataset, Maximum tree depth (max_depth), Minimum node size (min_size)\n",
    "Output: Decision Tree\n",
    "\n",
    "Procedure:\n",
    "\n",
    "1. Start with the entire training dataset.\n",
    "\n",
    "2. Determine the best feature and value to split the dataset on:\n",
    "   - For each feature in the dataset:\n",
    "     - For each unique value of that feature:\n",
    "       - Split the dataset into two groups based on whether their value for the feature is less than or equal to the value.\n",
    "       - Calculate the cost (such as Gini impurity for classification or sum of squared residuals for regression) of this split.\n",
    "       - Keep track of the feature and value that produces the lowest cost.\n",
    "\n",
    "3. Create a node in the tree representing this decision (to split on the best feature and value).\n",
    "\n",
    "4. Recursively apply this process to each of the two groups of data created by the split. Each group creates a new branch in the tree:\n",
    "   - If a group is pure (all the outputs are the same) or if it is smaller than min_size or if the tree depth is equal to max_depth, create a leaf node. The prediction of the leaf node is the most common output in the group (for classification) or the mean output (for regression).\n",
    "   - Otherwise, repeat from step 2 with the current group of data.\n",
    "\n",
    "5. Return the tree. For a new instance, to make a prediction, start at the root of the tree and follow the branches based on the instance's features, until a leaf node is reached. The prediction is the value associated with the leaf node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "    # count all samples at split point\n",
    "    n_instances = sum([len(group) for group in groups])\n",
    "    # sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        # avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        # score the group based on the score for each class\n",
    "        for class_val in classes:\n",
    "            p = (group[:, -1] == class_val).sum() / size\n",
    "            score += p * p\n",
    "        # weight the group score by its relative size\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    return gini\n",
    "\n",
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset, n_features):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    features = list()\n",
    "    while len(features) < n_features:\n",
    "        index = randrange(len(dataset[0])-1)\n",
    "        if index not in features:\n",
    "            features.append(index)\n",
    "    for index in features:\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left, n_features)\n",
    "        split(node['left'], max_depth, min_size, n_features, depth+1)\n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right, n_features)\n",
    "        split(node['right'], max_depth, min_size, n_features, depth+1)\n",
    "\n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "    root = get_split(train, n_features)\n",
    "    split(root, max_depth, min_size, n_features, 1)\n",
    "    return root\n",
    "\n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataset, ratio):\n",
    "    sample = list()\n",
    "    n_sample = round(len(dataset) * ratio)\n",
    "    while len(sample) < n_sample:\n",
    "        index = randrange(len(dataset))\n",
    "        sample.append(dataset[index])\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m         prediction \u001b[39m=\u001b[39m predict(tree, row)\n\u001b[1;32m     24\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPredicted=\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, Actual=\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (prediction, row[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]))\n\u001b[0;32m---> 26\u001b[0m test_CART()\n",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m, in \u001b[0;36mtest_CART\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m min_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[39m# build the tree\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m tree \u001b[39m=\u001b[39m build_tree(dataset, max_depth, min_size, n_features\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTree:\u001b[39m\u001b[39m'\u001b[39m, tree)\n\u001b[1;32m     21\u001b[0m \u001b[39m# make predictions\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 80\u001b[0m, in \u001b[0;36mbuild_tree\u001b[0;34m(train, max_depth, min_size, n_features)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_tree\u001b[39m(train, max_depth, min_size, n_features):\n\u001b[0;32m---> 80\u001b[0m     root \u001b[39m=\u001b[39m get_split(train, n_features)\n\u001b[1;32m     81\u001b[0m     split(root, max_depth, min_size, n_features, \u001b[39m1\u001b[39m)\n\u001b[1;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m root\n",
      "Cell \u001b[0;32mIn[6], line 43\u001b[0m, in \u001b[0;36mget_split\u001b[0;34m(dataset, n_features)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m dataset:\n\u001b[1;32m     42\u001b[0m     groups \u001b[39m=\u001b[39m test_split(index, row[index], dataset)\n\u001b[0;32m---> 43\u001b[0m     gini \u001b[39m=\u001b[39m gini_index(groups, class_values)\n\u001b[1;32m     44\u001b[0m     \u001b[39mif\u001b[39;00m gini \u001b[39m<\u001b[39m b_score:\n\u001b[1;32m     45\u001b[0m         b_index, b_value, b_score, b_groups \u001b[39m=\u001b[39m index, row[index], gini, groups\n",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m, in \u001b[0;36mgini_index\u001b[0;34m(groups, classes)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# score the group based on the score for each class\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m class_val \u001b[39min\u001b[39;00m classes:\n\u001b[0;32m---> 15\u001b[0m     p \u001b[39m=\u001b[39m (group[:, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m==\u001b[39m class_val)\u001b[39m.\u001b[39msum() \u001b[39m/\u001b[39m size\n\u001b[1;32m     16\u001b[0m     score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m p \u001b[39m*\u001b[39m p\n\u001b[1;32m     17\u001b[0m \u001b[39m# weight the group score by its relative size\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# Test CART on Banknote dataset\n",
    "def test_CART():\n",
    "    seed(1)\n",
    "    # create a small toy dataset\n",
    "    dataset = [[2.771244718,1.784783929,0],\n",
    "    [1.728571309,1.169761413,0],\n",
    "    [3.678319846,2.81281357,0],\n",
    "    [3.961043357,2.61995032,0],\n",
    "    [2.999208922,2.209014212,0],\n",
    "    [7.497545867,3.162953546,1],\n",
    "    [9.00220326,3.339047188,1],\n",
    "    [7.444542326,0.476683375,1],\n",
    "    [10.12493903,3.234550982,1],\n",
    "    [6.642287351,3.319983761,1]]\n",
    "    # set parameters\n",
    "    max_depth = 2\n",
    "    min_size = 1\n",
    "    # build the tree\n",
    "    tree = build_tree(dataset, max_depth, min_size, n_features=2)\n",
    "    print('Tree:', tree)\n",
    "    # make predictions\n",
    "    for row in dataset:\n",
    "        prediction = predict(tree, row)\n",
    "        print('Predicted=%d, Actual=%d' % (prediction, row[-1]))\n",
    "\n",
    "test_CART()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-0.4.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
