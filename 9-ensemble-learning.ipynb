{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emsemble Learning\n",
    "\n",
    "Reference:\n",
    "- element of statistical learning\n",
    "\n",
    "Two basic ensemble methods are:\n",
    "- Bagging: training a bunch of individual models in a parallel way. Each model is trained by a random subset of the data. The final predictions are averaged for regression or vote for classification.\n",
    "- Boosting: training a bunch of individual models in a sequential way. Each individual model learns from mistakes made by the previous model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "\n",
    "**Framework of boosting**:\n",
    "- obtain the first classifier $f_1(x)$\n",
    "- find other models $f_2(x)$ to help $f_1(x)$\n",
    "  - if similar, will not help a lot\n",
    "  - how to obtain $f_2(x)$\n",
    "- obtain the second classifier $f_2(x)$\n",
    "- finally combine all the classifiers\n",
    "\n",
    "The classifiers are learned sequentially\n",
    "\n",
    "\n",
    "**How to obtain different classifiers**\n",
    "- re-sample training data to form a new set\n",
    "- re-weight training data to form a new set\n",
    "- in real implementation, only need to change the cost/objective function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Trees\n",
    "\n",
    "An typical boosting algorithm is AdaBoost (Adaptive Boosting). AdaBoost works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. New classifiers are added sequentially that focus their training on the more difficult patterns.\n",
    "\n",
    "- **Idea**: train $f_2(x)$ on the new training set that fails $f_1(x)$\n",
    "- **how to find a new trainig set that fails $f_1(x)$ ?**\n",
    "    - if $x_i$ is misclassified by $f_1$, \n",
    "      - $w_{i,2} = w_{i, 1} * d_1$\n",
    "    - if correctly classified:\n",
    "      - $w_{i,2} = w_{i, 1} / d_1$\n",
    "    - $d_1$ should be greater than 1, how to find $d_1$?\n",
    "\n",
    "\n",
    "$$\n",
    "err_1 = \\frac{\\sum_i^N w_{1, i} I(y_i \\neq f_1(x_i))}{\\sum_i^N w_{i, 1}} \n",
    "$$\n",
    "\n",
    "$$\n",
    "err_1 < 0.5\n",
    "$$\n",
    "\n",
    "\n",
    "changing the sample weights from $u_{i, 1}$ to $w_{i, 2}$ such that\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_i^N w_{2, i} I(y_i \\neq f_1(x_i))}{\\sum_i^N w_{i, 2}} = 0.5 \n",
    "$$\n",
    "\n",
    "We can calculate $d_1$ based on this assumption, as\n",
    "\n",
    "- mismatched: $w_{i, 2} = w_{i, 1} d_1 = w_{i, 1} \\exp^{\\alpha_1}$\n",
    "- else: $w_{i, 2} = w_{i, 1} / d_1 = w_{i, 1} \\exp^{-\\alpha_1}$\n",
    "\n",
    "where $d1 = \\sqrt{\\frac{1-err_1}{err_1}}$, $\\alpha_1 = \\ln d_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![adboost](./resources/imgs/adaboost.png)\n",
    "\n",
    "\n",
    "AdaBoost is equivalent to a forward stagewise additive modeling using exponential loss.\n",
    "Since exponential loss is sensitive to outliers, AdaBoost has been empirically observed to be quite sensitive to noisy data and outliers.\n",
    "Typical classification loss function like cross-entroy, hinge loss, etc. are more robust to outliers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Trees\n",
    "\n",
    "\n",
    "\n",
    "![gbt](./resources/imgs/gradient-boosting-tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Pseudocode for Gradient Boosting Algorithm\n",
    "\n",
    "# Step 1: Initialize the model\n",
    "Initialize the model with a constant value, such as the mean of the target variable\n",
    "\n",
    "# Step 2: Set the number of iterations (M)\n",
    "Set the number of iterations (M) for the boosting algorithm\n",
    "\n",
    "# Step 3: Iterate M times\n",
    "for m in range(M):\n",
    "    # Step 4: Compute the pseudo-residuals\n",
    "    Compute the negative gradient of the loss function with respect to the current model's predictions\n",
    "    This represents the pseudo-residuals, which are the errors that the current model is not able to capture\n",
    "    \n",
    "    # Step 5: Fit a weak learner to the pseudo-residuals\n",
    "    Fit a weak learner (e.g., decision tree) to the pseudo-residuals\n",
    "    The weak learner should be trained to minimize the loss function\n",
    "    \n",
    "    # Step 6: Update the model\n",
    "    Update the model by adding the weak learner's predictions, scaled by a learning rate (alpha), to the current model's predictions\n",
    "    \n",
    "# Step 7: Return the final model\n",
    "Return the final boosted model\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
