{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference\n",
    "\n",
    "A central task in the application of probabilistic models is the evaluation of the posterior distribution $p(Z|X)$ of the latent variables $Z$ given the observed (visible) data variables $X$, and the evaluation of expectations computed with respect to this distribution, such as in the EM algorithm. \n",
    "The model might also contain some deterministic parameters, which we\n",
    "will leave implicit for the moment, or it may be a fully Bayesian model in which any\n",
    "unknown parameters are given prior distributions and are absorbed into the set of\n",
    "latent variables denoted by the vector $Z$.\n",
    "\n",
    " \n",
    "In Gaussian mixture models and hidden Markov models, the posterior distribution can be mathematically calculated due to the special format of the assumed distributions, although the computational cost is exponential in the number of latent variables\n",
    "In many other cases, the posterior distribution $p(Z|X)$ is intractable because the the dimensionality of the latent space is too high to work with directly or because the posterior distribution has a highly complex form for which the expectations are not analytically traceable.\n",
    "\n",
    "In such cases, we need to resort to approximation schemes. \n",
    "The approximation can be stochastic approximation or deterministic approximation.\n",
    "Stochastic approximation methods are based on sampling from the posterior distribution, and deterministic approximation methods are based on finding a tractable distribution that is close to the posterior distribution in some sense.\n",
    "Stochastic approximation methods are often more accurate than deterministic approximation methods, but they are also more computationally expensive. \n",
    "One example is Markov chain Monte Carlo (MCMC) methods.\n",
    "Deterministic approximation methods are often faster than stochastic approximation methods, but they are also less accurate.\n",
    "One example is variational inference.\n",
    "we can use variational inference to approximate the posterior distribution $p(Z|X)$ by a simpler distribution $q(Z)$, chosen from a tractable family of distributions. The approximation is obtained by minimizing the Kullback-Leibler divergence between $q(Z)$ and $p(Z|X)$, which is equivalent to maximizing a lower bound on the marginal log-likelihood $\\log p(X)$.\n",
    "\n",
    "\n",
    "Variational methods restricts the range of functions over which the optimization is performed, for instance by considering only quadratic functions. \n",
    "In the application of probablistic inference, the restriction may for example take the form of factorization assumptions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From EM, we have \n",
    "\n",
    "$$\\log p(X) = \\mathcal{L}(q) + KL(q||p)$$\n",
    "$$ \\mathcal{L}(q) = \\int q(Z) \\log \\frac{p(X,Z)}{q(Z)} dZ$$\n",
    "$$ KL(q||p) = - \\int q(Z) \\log \\frac{p(Z|X)}{q(Z)} dZ$$\n",
    "\n",
    "where $\\mathcal{L}(q)$ is the evidence lower bound (ELBO) and $KL(q||p)$ is the Kullback-Leibler divergence between $q(Z)$ and $p(Z|X)$.\n",
    "\n",
    "Here we dont explicitly write the dependence on the parameter $\\theta$ as we assume the parameters are stochastic variables and are absorbed into the latent variables $Z$ with given prior distributions. \n",
    "This is a typical Bayesian approach.\n",
    "We also sued integration instead of summation as the latent variables are continuous variables here.\n",
    "\n",
    "If the posterior distribution $p(Z|X)$ is intraceable, the EM algorithm cannot be applied directly.\n",
    "Usign varitional inference, we can consider a restricted family of distributions $q(Z)$, and then seek the member of this family for which the KL divergence is minimized.\n",
    "The goal is to restrict the family sufficiently that they comprimise only tractable distributions, while at the mean time allowing the family to be sufficiently flexible that it can provide a good approximation to the true posterior distribution.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Gaussian mixture model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
