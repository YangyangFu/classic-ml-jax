{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Gaussian Process\n",
    "\n",
    "In Bayesian regression, we assumed linear relationship between the output and the inputs or the tranformation of inputs. \n",
    "- Given training data set $\\mathcal{D} = \\{(X_n, T_n)\\}_{n=1}^N$, we want to predict the output target $t$ for a new input $\\mathcal{x}$.\n",
    "    - Inference: get the posterior distribution $p(w|X, T)$\n",
    "    - Decision: get the posterior predictive distribution $p(t|x, X, T)$ for a new input $\\mathcal{x}$\n",
    "\n",
    "**Inference**:\n",
    "- assume a linear model $t = y(x, w) = w^Tx + \\epsilon$, where $\\epsilon$ is the modeling noise, $y$ is the model to be learned.\n",
    "- assume a prior distribution for parameters $w$, e.g., $p(w) = N(w|0, I)$\n",
    "- assume a likelihood function $p(T|X, w)$, e.g., $p(T|X, w) = N(T|y(X), \\beta^{-1}I) = N(T|Xw, \\beta^{-1}I)$, where $\\beta$ is the precision of the modeling noise. The unknown parameters are $w$ and $\\beta$, which can be solved by maximizing the logrithatic likelihood function $p(T|X, w)$.\n",
    "    - $p(T|X, w) = \\prod_{n=1}^N N(T_n|X_nw, \\beta^{-1})$\n",
    "    - $\\log p(T|X, w) = -\\frac{\\beta}{2}\\sum_{n=1}^N (T_n - X_nw)^2 + \\frac{N}{2}\\log \\beta - \\frac{N}{2}\\log (2\\pi)$\n",
    "    - by solving the optimization problem, we can get the $w$ and $\\beta$ that maximize the likelihood \n",
    "- assume a posterior distribution $p(w|X, T)$, which is propotional ot the product of the prior distribution and the likelihood. Due to the choice of a conjugate Gaussian prior distribution, the posterior distribution is alos a Gaussian distribution. Let's assume $p(w|X, T) = N(w|m_N, S_N)$ after observing $N$ data from training set. The mean $m_N$ and variance $S_N$ can be calculated based on marginal and conditional Gaussian principles as follows:\n",
    "    - $p(w|X, T) = N(w|m_N, S_N)$\n",
    "    - $S_N^{-1} = \\beta XX^T + I$\n",
    "    - $m_N = \\beta S_N X^T T$\n",
    "\n",
    "**Decision**\n",
    "- based on conditional and marginal probability rules, the posterior predictive distribution $p(t|x,X,T)$ can be expressed as:\n",
    "    - $p(t|x,X,T) = \\int p(t|x,w)p(w|X,T)dw$\n",
    "    - $p(t|x,X,T) = N(t|m_N^Tx, \\sigma_N^2(x))$\n",
    "    - $\\sigma_N^2(x) = \\beta^{-1} + x^TS_Nx$\n",
    "\n",
    "\n",
    "Gaussian process is very similar to the above process, but there are also differences:\n",
    "- GP instead of inferencing the parameters of a given linear model $y(x) = w^Tx$, infers the function itself $y(x)$ directly from the training data.\n",
    "- GP is a non-parametric approach, which means that the number of parameters grows with the size of the training data set.\n",
    "- GP is a distribution over functions, which is defined by a mean function $m(x)$ and a covariance function $k(x, x')$.\n",
    "- GP is a memory-based approach, like k-NN, which means it stores the training data and uses all of them to predict the output for a new input. It will require retraining the model when new data is added to the training set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Gaussian Process Regression\n",
    "\n",
    "The regression problem is standard:\n",
    "- given training set $(X, T)$, predict the output $t$ for a new input $x$.\n",
    "\n",
    "**Inference**\n",
    "- assume the prediction is made by a Gaussian process $y(x)$ so that $t_i = y(x_i) + \\epsilon_i$ for all the data points in the training set, where $\\epsilon_i$ is the modeling noise.\n",
    "- get conditional distribution $p(T|y(X)) = N(T|y(X), \\beta^{-1}I_N)$\n",
    "- assume the Gaussian process prior or marginal distribution $p(y(X)) = N(y(X)|m(X), k(X, X'))$, where $m(X)$ is the mean function and $k(X, X')$ is the covariance function, also known as kernel function.\n",
    "    - we can always assume 0 mean for prior distribution, i.e., $p(y) = N(y|0, K)$, where $K$ is $k(X, X')$\n",
    "- find the conditional distribution $p(T|X)$:\n",
    "    - $p(T|X) = \\int_{y} p(T, y|X)dy = \\int_{y} p(T|y,X)p(y|X)dy$\n",
    "    - $p(T|X) = N(T|0, C_N)$\n",
    "    - $C_N = K_N + \\beta^{-1}I_N$ -> simply add the covariance due to independency\n",
    "\n",
    "**Decision**\n",
    "- find the posterior distribution $p(t|x, X, T)$:\n",
    "    - get joint distribution $p(t, T|x, X)$ based on conditional distribution $p(T|X)$ -> simply added one more data point\n",
    "        - $p(t, T|x, X) = N(t,T|0, C_{N+1})$\n",
    "        - $ C_{N+1} = \\begin{bmatrix}\n",
    "                        C_N & \\mathcal{k} \\\\\n",
    "                        \\mathcal{k}^T & c\n",
    "                        \\end{bmatrix} \n",
    "          $ -> (N+1)-by-(N+1) matrix\n",
    "        - $c = k(x, x') + \\beta^{-1}, \\mathcal{k} = k(x_N, x_{N+1}) = k(X[-1], x)$\n",
    "    - get conditional distribution $p(t|x, X, T)$ based on joint distribution $p(t, T|x, X)$\n",
    "        - $p(t|x, X, T) = N(t|m(x), \\sigma^2(x))$\n",
    "        - $m(x) = \\mathcal{k}^TC_N^{-1}T$\n",
    "        - $\\sigma^2(x) = c - \\mathcal{k}^TC_N^{-1}\\mathcal{k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.scipy as jsp\n",
    "import jax.numpy as jnp \n",
    "\n",
    "# kernel\n",
    "class GaussianProcessRegression():\n",
    "    def rbf_kernel(self, params, x1, x2):\n",
    "        # take a squared exponential kernel as an example\n",
    "        return params['variance'] * jnp.exp(-0.5 * params['length_scale'] * jnp.sum((x1 - x2)**2))\n",
    "\n",
    "    def construct_covariance_matrix(self, params, X):\n",
    "        # assume first dimension is the batch dimension\n",
    "        kernel_vmap = jax.vmap(jax.vmap(self.rbf_kernel, in_axes=(None, 0, 0)), in_axes=(None, 0, 0))\n",
    "        K = kernel_vmap(params, X[:, None, :], X[None, :, :])\n",
    "        return K\n",
    "    \n",
    "    def compute_posterior(self, params, X_train, y_train, X_test, noise_variance):\n",
    "        # compute convariance matrices C_{n+1} = [C_n, k; k.T, c_{n+1}]\n",
    "        K = self.construct_covariance_matrix(params, X_train)\n",
    "        K_s = self.construct_covariance_matrix(params, X_test)\n",
    "        K_sT = self.construct_covariance_matrix(params, X_test, X_train)\n",
    "        \n",
    "        # directly computing the inverse is numerically unstable\n",
    "        # K_inv = jnp.linalg.inv(K + noise_variance * jnp.eye(K.shape[0]))\n",
    "        # we use scipy.linalg.solve instead\n",
    "        # K*K_inv = I\n",
    "        # refer to Eq.(6.62) in PRML\n",
    "        K_inv = jsp.linalg.solve(K + noise_variance * jnp.eye(len(X_train)), jnp.eye(len(X_train)))\n",
    "\n",
    "        # posterior mean\n",
    "        mu_s = jnp.matmul(jnp.matmul(K_sT, K_inv), y_train)\n",
    "        \n",
    "        # posteiror covariance\n",
    "        cov_s = K_s - jnp.matmul(jnp.matmul(K_sT, K_inv), K_sT.T)\n",
    "        \n",
    "        # diagonal elements of the covariance matrix\n",
    "        var_s = jnp.diag(cov_s) \n",
    "        \n",
    "        # return \n",
    "        return mu_s, var_s\n",
    "        \n",
    "    # Make predictions\n",
    "    def predict(self, params, X_train, y_train, X_test, noise_variance):\n",
    "        mu_s, cov_s = self.compute_posterior(params, X_train, y_train, X_test, noise_variance)\n",
    "        return mu_s, cov_s\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Shapes must be 1D sequences of concrete values of integer type, got (None, 10).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/jax-0.4.8/lib/python3.10/site-packages/jax/_src/core.py:1905\u001b[0m, in \u001b[0;36m_dim_handler_and_canonical\u001b[0;34m(*dlist)\u001b[0m\n\u001b[1;32m   1904\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1905\u001b[0m   canonical\u001b[39m.\u001b[39mappend(operator\u001b[39m.\u001b[39;49mindex(d))\n\u001b[1;32m   1906\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m# Perform Gaussian Process Regression\u001b[39;00m\n\u001b[1;32m     20\u001b[0m gpr \u001b[39m=\u001b[39m GaussianProcessRegression()\n\u001b[0;32m---> 21\u001b[0m mu_s, cov_s \u001b[39m=\u001b[39m gpr\u001b[39m.\u001b[39;49mpredict(params, X_train, y_train, X_test, noise_variance)\n\u001b[1;32m     23\u001b[0m \u001b[39m# Plot the results\u001b[39;00m\n\u001b[1;32m     24\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n",
      "Cell \u001b[0;32mIn[8], line 44\u001b[0m, in \u001b[0;36mGaussianProcessRegression.predict\u001b[0;34m(self, params, X_train, y_train, X_test, noise_variance)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, params, X_train, y_train, X_test, noise_variance):\n\u001b[0;32m---> 44\u001b[0m     mu_s, cov_s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_posterior(params, X_train, y_train, X_test, noise_variance)\n\u001b[1;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m mu_s, cov_s\n",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m, in \u001b[0;36mGaussianProcessRegression.compute_posterior\u001b[0;34m(self, params, X_train, y_train, X_test, noise_variance)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_posterior\u001b[39m(\u001b[39mself\u001b[39m, params, X_train, y_train, X_test, noise_variance):\n\u001b[1;32m     18\u001b[0m     \u001b[39m# compute convariance matrices C_{n+1} = [C_n, k; k.T, c_{n+1}]\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     K \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct_covariance_matrix(params, X_train)\n\u001b[1;32m     20\u001b[0m     K_s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstruct_covariance_matrix(params, X_test)\n\u001b[1;32m     21\u001b[0m     K_sT \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstruct_covariance_matrix(params, X_test, X_train)\n",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mGaussianProcessRegression.construct_covariance_matrix\u001b[0;34m(self, params, X)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstruct_covariance_matrix\u001b[39m(\u001b[39mself\u001b[39m, params, X):\n\u001b[1;32m     12\u001b[0m     \u001b[39m# assume first dimension is the batch dimension\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     kernel_vmap \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mvmap(jax\u001b[39m.\u001b[39mvmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrbf_kernel, in_axes\u001b[39m=\u001b[39m(\u001b[39mNone\u001b[39;00m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)), in_axes\u001b[39m=\u001b[39m(\u001b[39mNone\u001b[39;00m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m))\n\u001b[0;32m---> 14\u001b[0m     K \u001b[39m=\u001b[39m kernel_vmap(params, X[:, \u001b[39mNone\u001b[39;49;00m, :], X[\u001b[39mNone\u001b[39;49;00m, :, :])\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m K\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax-0.4.8/lib/python3.10/site-packages/jax/_src/core.py:1907\u001b[0m, in \u001b[0;36m_dim_handler_and_canonical\u001b[0;34m(*dlist)\u001b[0m\n\u001b[1;32m   1905\u001b[0m       canonical\u001b[39m.\u001b[39mappend(operator\u001b[39m.\u001b[39mindex(d))\n\u001b[1;32m   1906\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m-> 1907\u001b[0m       \u001b[39mraise\u001b[39;00m _invalid_shape_error(dlist)\n\u001b[1;32m   1909\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(special_handlers) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1910\u001b[0m   msg \u001b[39m=\u001b[39m (\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDimension size operation involves multiple special dimension types \u001b[39m\u001b[39m{\u001b[39;00mdlist\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Shapes must be 1D sequences of concrete values of integer type, got (None, 10)."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from jax import random\n",
    "import jax.numpy as jnp \n",
    "\n",
    "# Generate some example data\n",
    "key = random.PRNGKey(0)\n",
    "X_train = random.uniform(key, (10, 1), minval=-5, maxval=5)\n",
    "y_train = jnp.sin(X_train) + 0.2 * random.normal(key, (10, 1))\n",
    "\n",
    "# Define the kernel parameters\n",
    "params = {'variance': 1.0, 'length_scale': 1.0}\n",
    "\n",
    "# Define the noise variance\n",
    "noise_variance = 0.05\n",
    "\n",
    "# Generate some test points\n",
    "X_test = jnp.linspace(-5, 5, 100)[:, None]\n",
    "\n",
    "# Perform Gaussian Process Regression\n",
    "gpr = GaussianProcessRegression()\n",
    "mu_s, cov_s = gpr.predict(params, X_train, y_train, X_test, noise_variance)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.plot(X_train, y_train, 'kx')\n",
    "plt.plot(X_test, mu_s, 'r')\n",
    "plt.fill_between(X_test.flatten(), mu_s - jnp.sqrt(jnp.diag(cov_s)), mu_s + jnp.sqrt(jnp.diag(cov_s)), color='red', alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-0.4.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
