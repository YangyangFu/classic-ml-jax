{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning\n",
    "\n",
    "A deep neural network is used to extract feature for a linear basis regression. This avoids mannualy design/select features for a given problem.\n",
    "\n",
    "Typical questions to ask:\n",
    "- how many layers? how many neurons for each layer?\n",
    "  - trial and errors + intuition  \n",
    "- can the structure be automatically determined?\n",
    "  - yes. some research are ongoing, but not widely applied. For example, evolutionary artificial neural networks\n",
    "- can we design the network structure?\n",
    "  - yes. Most of popular deep neural network models have their own structure design, such as residual network, recurrent network, convolution networks, etc. All these models are not fully connected networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Backpropagation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Chain Rule\n",
    "\n",
    "**Case 1**: \n",
    "\n",
    "given $y = g(x)$, $z=h(y)$, we can do $\\Delta x \\rightarrow \\Delta y \\rightarrow \\Delta z$, which results in: $\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}$\n",
    "\n",
    "\n",
    "**Case 2**\n",
    "\n",
    "given $x=g(s), y=h(s), z=k(x,y)$, we have $\\frac{dz}{ds} = \\frac{\\alpha z}{\\alpha x} \\frac{dx}{ds} + \\frac{\\alpha z}{\\alpha y}\\frac{dy}{ds}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Forward and Backward Pass\n",
    "\n",
    "For a neural network, forward pass is to calculate the output of the network given the input. Backward pass is to calculate the gradient of the loss function with respect to the parameters of the network.\n",
    "\n",
    "We define a neural netowrk as follows.\n",
    "\n",
    "$x$: input\n",
    "$x_i$: input for the $i$-th layer\n",
    "$z_i$: input for the activation functions $\\delta_i$\n",
    "$w_i$: weight for the $i$-th layer\n",
    "$\\delta_i$: activation function for the $i$-th layer\n",
    "$\\mathcal{L}$: loss function\n",
    "Therefore,\n",
    "\n",
    "we have:\n",
    "\n",
    "$$ z_i = w_i x_{i-1}$$\n",
    "$$ x_{i+1} = \\delta_i(z_i)$$\n",
    "\n",
    "Procedure: start from the output layer, backpropagate the gradient to the input layer.\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial w_i} = \\frac{\\partial \\mathcal{L}}{\\partial x_{i+1}} \\frac{\\partial x_{i+1}}{\\partial z_i} \\frac{\\partial z_i}{\\partial w_i}$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tips for Deep Learning\n",
    "The overall decision tree for tuning a deep learning model is shown in the following figure.\n",
    "Reference: https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/overfit-v6.pdf\n",
    "\n",
    "![training-tips](./resources/imgs/training-tips.png)\n",
    "\n",
    "The training does not perform well, then:\n",
    "- large training loss\n",
    "    - underfit: model bias is large -> model is too simple\n",
    "    - optimization failures\n",
    "- small training loss, large validation loss\n",
    "    - overfit: model variance is large -> model is too complex\n",
    "    - mismatch\n",
    "- small training loss, small validation loss\n",
    "    - good model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 How to Debug\n",
    "\n",
    "**Model Bias**\n",
    "Underfitting -> model bias is large -> model is too simple\n",
    "- increase model complexity to check on the training loss\n",
    "- if the model complexity is enough, then we should see the training loss is small. The validation loss after complexity increase might be large. We can stop adding complexity till this point.\n",
    "\n",
    "**Optimization Failures**\n",
    "- gain insights from comparisons: compare with a more complex model\n",
    "- start from a shallower networks or other models which are easier to optimize\n",
    "- if deeper networks do not obtain smaller loss on training data, then there are optimzation failures.\n",
    "\n",
    "Solutions:\n",
    "- critical points\n",
    "  - smaller batch\n",
    "  - momentum\n",
    "  - adaptive learning rate\n",
    "- vanishing gradients\n",
    "  - change of activation function\n",
    "  - skip connections\n",
    "- exploding gradients\n",
    "  - gradient clipping\n",
    "  - smaller learning rate\n",
    "\n",
    "**Model Variance**\n",
    "Overfitting -> model variance is large -> model is too complex\n",
    "- small training loss, large validation loss\n",
    "\n",
    "Solutions:\n",
    "- reduce model complexity (trade-off between bias and variance, may use cross-validation)\n",
    "  - **why cross-validation can reduce variance? isn't it just a way to evaluate the model?**\n",
    "    - perform cross-validation on a set of models, and choose the one with the smallest validation loss\n",
    "  - less pameters: smaller model, sharing parameters\n",
    "  - less features\n",
    "  - early stopping\n",
    "  - regularization\n",
    "  - dropout\n",
    "- more training data\n",
    "  - data augmentation\n",
    "\n",
    "**Mismatch**\n",
    "The training data and testing data have different distributions.\n",
    "How to detect the distribution mismatch?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 When gradient is small\n",
    "\n",
    "The training process is not always successful. It may fail to converge, or converge to a bad local minimum.\n",
    "When the loss is not small enough, most likely the optimization fails becuase of critical points such as local minima or saddle points.\n",
    "- we can check gradients to see if they are close to zero. If they are close to zero, then it is likely to be a critical point.\n",
    "- local minima is difficult to escape.\n",
    "- saddle point can be easily escaped. However, the gradient is small, and the training is slow.\n",
    "\n",
    "\n",
    "How to know if it is due to saddle points or local minima since the explicit shape of the loss function is difficult/impossible to draw?\n",
    "We can use Taylor series expansion to have a look at the loss function space around the critical point.\n",
    "\n",
    "Say after a few of epochs of training, the loss function is $\\mathcal{L}(w)$. We can use Taylor series expansion to approximate the loss function around $w$.\n",
    "Define a very small pertubation $\\Delta w$, then we have\n",
    "\n",
    "$$ \\mathcal{L}(w + \\Delta w) \\approx \\mathcal{L}(w) + \\Delta w^T \\nabla \\mathcal{L}(w) + \\frac{1}{2} \\Delta w^T H \\Delta w$$\n",
    "\n",
    "where $H$ is the Hessian matrix of the loss function at $w$.\n",
    "The gradient is zero at the critical point, thus the first order term is zero, which leads to:\n",
    "\n",
    "$$ \\mathcal{L}(w + \\Delta w) \\approx \\mathcal{L}(w) + \\frac{1}{2} \\Delta w^T H \\Delta w$$\n",
    "\n",
    "- If $H$ is positive definite (e.g., $\\Delta w^T H \\Delta w > 0$), then for all $\\Delta w$ around $w$, the $\\mathcal{L}(w+\\Delta w) > \\mathcal{L}(w)$, which means the critical point at $w$ is a local minimum.\n",
    "- If $H$ is negative definite (e.g., $\\Delta w^T H \\Delta w < 0$), then for all $\\Delta w$ around $w$, the $\\mathcal{L}(w+\\Delta w) < \\mathcal{L}(w)$, which means the critical point at $w$ is a local maximum.\n",
    "- If $H$ is indefinite (e.g., $\\Delta w^T H \\Delta w > 0$ and $\\Delta w^T H \\Delta w < 0$), then for some $\\Delta w$ around $w$, the $\\mathcal{L}(w+\\Delta w) > \\mathcal{L}(w)$, and for some $\\Delta w$ around $w$, the $\\mathcal{L}(w+\\Delta w) < \\mathcal{L}(w)$, which means the critical point at $w$ is a saddle point.\n",
    "\n",
    "A positive definite matrix has all positive eigenvalues, and a negative definite matrix has all negative eigenvalues.\n",
    "\n",
    "\n",
    "If the critical point is a saddle point, then it is easy to get out by move along the direction of the negative eigenvalue.\n",
    "\n",
    "Proof:\n",
    "\n",
    "- let $H = Q \\Lambda Q^T$, where $\\Lambda$ is a diagonal matrix with eigenvalues on the diagonal, and $Q$ is the eigenvector matrix.\n",
    "- if we use the eigenvector as the perturbation of gradient, then we have $\\Delta w^T H \\Delta w = Q^THQ = Q^T\\lambda Q = \\lambda Q^TQ$\n",
    "- With the Taylor expansion, then we have $\\mathcal{L}(w + \\Delta w) \\approx \\mathcal{L}(w) + \\frac{1}{2} \\lambda Q^TQ = \\mathcal{L}(w) + \\frac{1}{2} \\lambda$\n",
    "  - if we want to escape the saddle point and move towards a minimum value, then we use the negative eigenvalue, which leads to $\\mathcal{L}(w + \\Delta w) < \\mathcal{L}(w)$\n",
    "  - if we want to escape the saddle point and move towards a maximum value, then we use the positive eigenvalue, which leads to $\\mathcal{L}(w + \\Delta w) > \\mathcal{L}(w)$\n",
    "\n",
    "But this method is rarely used in practice due to the high computational cost of computing the Hessian matrix and its eigenvalues for deep models.\n",
    "\n",
    "How easy will a model be stuck in local minima or saddle points?\n",
    "- local minima is rare in high dimensional space. most time the hessian matrix is indefinite. we can check the ratio of positive eigenvalues of the hessian matrix after training. Mostly likely we will find the ratio is close to 0.5.\n",
    "\n",
    "**(1). Batch size**\n",
    "\n",
    "Batch: use a batch of data to compute the gradient, and then update the parameters.\n",
    "\n",
    "Why batch?\n",
    "- large batch vs small batch \n",
    "  - small batch requires more time for one epoch, gradient is more noisy -> better optimization and generation\n",
    "  - large batch requires less time for one epoch, gradient is less noisy -> worse training accuracy due to optimziation failure\n",
    "  - in terms of training time with parallel computation, a good batch size is ~100-1000\n",
    "  - in terms of training accuracy, noisy gradient is better than less noisy gradient\n",
    "\n",
    "Thus, smaller batch size is may help escape critical points due to noisy gradient.\n",
    "\n",
    "**(2). Adaptive learning rate**\n",
    "\n",
    "See in next section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3 Gradient Vanishing\n",
    "\n",
    "**Gradient vanishing**: \n",
    "the gradient becomes smaller and smaller as the number of layers increases. \n",
    "This is because the gradient is the product of the gradients of each layer. If the gradient of each layer is smaller than 1, the product will be smaller and smaller.\n",
    "Thus, the output layer may have large gradients, learns fast and already converge, but the first few layers have small gradients, learns slow and almost random.\n",
    "This leads to that the deeper the network, the harder the training.\n",
    "\n",
    "This is usually caused by `Sigmoid` function as the activation. \n",
    "`Sigmoid` function compress the infinite range of real numbers to a finite range, which is $[0,1]$. \n",
    "From the forward passing point of view, at the first layer, even if we add a large gradient pertubation $\\Delta w$, (then $\\Delta z$ is large), after the `Sigmoid` function, the changes of `Sigmoid` function outputs $\\Delta x$ is small.\n",
    "After a few laryers of `Sigmoid` function, the large gradient pertubation on the loss function will vanishe.\n",
    "This means the loss is not sensitive to the weights of the first few layers.\n",
    "\n",
    "Since the first few layer has small gradients, and the output layers have large gradients, we can use adaptive learning rate to solve this problem. For example, we can use Adam optimizer.\n",
    "\n",
    "Since `Sigmoid` function has such limiations, what are good alternatives?\n",
    "\n",
    "- changing of activation functions \n",
    "  \n",
    "***Relu***: fast and easy to compute, and can be considered as an infinite number of Sigmoid functions. \n",
    "\n",
    "$$ f(x) = \\begin{cases} x & x > 0 \\\\ 0 & x \\leq 0 \\end{cases}$$\n",
    "\n",
    "Relu will not use neurons that have negative gradients, thus it will lead to a thinner linear network.\n",
    "However, it has a problem called \"dead neuron\". If the input is negative, the gradient is zero. This means the neuron is dead. This is not a problem for the first few layers, but it is a problem for the last few layers. This is because the last few layers are close to the output layer, and the output layer has large gradients. If the last few layers are dead, the output layer will not learn anything.\n",
    "\n",
    "***Leaky Relu/Parametric Relu***: \n",
    "\n",
    "$$ f(x) = \\begin{cases} x & x > 0 \\\\ \\alpha x & x \\leq 0 \\end{cases}$$\n",
    "\n",
    "where $\\alpha$ is a small positive number, such as 0.01.\n",
    "\n",
    "\n",
    "***Maxout***: learnable activation function. can represent any activation functions, such as Relu.\n",
    "\n",
    "$$ f(x) = \\max(w_1^T x + b_1, w_2^T x + b_2)$$\n",
    "\n",
    "where $w_1, w_2, b_1, b_2$ are learnable parameters. If $w_2, b_2$ are zero, it is Relu.\n",
    "\n",
    "- activation function in maxout network can be any piecewise linear convex function\n",
    "- how many pieces depends on how many elements in a group. For example, if there are 3 elements in a group, then the activation function is a piecewise linear convex function with 3 pieces.\n",
    "\n",
    "How to differentiate throught a $max()$ operator?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Error surface is rugged\n",
    "\n",
    "Training can be difficult even without critical points. The error surface can be rugged, which means the loss function is not smooth.\n",
    "In this case, the learning rate cannot be one-size-fits-all. We need to use adaptive learning rate.\n",
    "\n",
    "Training stuck is not equal to small gradient. \n",
    "There are cases that the gradient is big but the training is stuck.\n",
    "  - the gradient may be big, thus the update direction is bouncing around the critical point, which leads to slow training.\n",
    "  - different parameters may need different learning rates. If the gradient at some direction is too small, we wish the learning rate is big. But if the gradient at some direction is too big, we wish the learning rate is small. Thus, we need to adjust the learning rate for different parameters.\n",
    "    - can be achieved by using RMSProp or Adam.\n",
    "  \n",
    "**(1). Adaptive learning rate**:\n",
    "\n",
    "- different updates for different parameters based on the magnitude of the gradient, such as RMSProp and Adam.\n",
    "- learning rate decay: decrease the learning rate as the training goes on.\n",
    "- warm up: start with a small learning rate, and increase the learning rate, and then decrease the learning rate as the training goes on.\n",
    "  - not fully explored yet.\n",
    "  - one explanation is during warmup period, the $\\delta$ is not accurate enough.\n",
    "\n",
    "***RMSProp***: use previous decayed gradients to adjust the learning rate. Only uses the magnititude of the previous gradient, no direction information i used.\n",
    "\n",
    "$$ w^1 \\leftarrow w^0 - \\frac{\\eta}{\\sqrt{v^0 + \\epsilon}} g^0$$\n",
    "$$ w^2 \\leftarrow w^1 - \\frac{\\eta}{\\sqrt{v^1 + \\epsilon}} g^1, \\hspace{5pt} v^1 = \\alpha(v^0)^2 + (1-\\alpha)(g^1)^2$$\n",
    "\n",
    "where $v^0$ is the moving average of the square of the gradient, and $\\epsilon$ is a small number to avoid division by zero.\n",
    "\n",
    "\n",
    "***Adam***: momentum + RMSProp, considers the previous movement, which is generally considers all the past gradients, including the direction information.\n",
    "\n",
    "- start at point $w^0$\n",
    "- movement of last step: $v^0 = 0$\n",
    "- compute gradient at $w^0$: $g^0 = \\Delta \\mathcal{L} (w^0)$\n",
    "- movement $v^1 = \\lambda v^0 -\\eta g^0$\n",
    "- move to $w^1 = w^0 + v^1$\n",
    "- compute gradient at $w^1$: $g^1 = \\Delta \\mathcal{L} (w^1)$\n",
    "- movement $v^2 = \\lambda v^1 -\\eta g^1$\n",
    "- move to $w^2 = w^1 + v^2$\n",
    "\n",
    "where $\\lambda$ is the momentum, and $\\eta$ is the learning rate.\n",
    "\n",
    "\n",
    "**(2). Change loss function**\n",
    "\n",
    "Cross entropy loss: $-\\sum_i y_i \\log \\hat{y}_i$\n",
    "\n",
    "Softmax loss: $-\\log \\frac{e^{y_i}}{\\sum_j e^{y_j}}$ -> Pytorch implementation is very interesting. when call `cross_entropy`, it will first apply softmax to the input, and then compute the cross entropy loss. Therefore no need add softmax layer for the output layer.\n",
    "\n",
    "The loss function can affect the difficulty of training.\n",
    "\n",
    "**(3). Batch normalization**\n",
    "\n",
    "The error space is rugged, and the gradient is not smooth. Batch normalization can smooth the error space and make the gradient smoother.\n",
    "\n",
    "Batch normalization is a layer that normalizes the input of the layer to have zero mean and unit variance.\n",
    "This leads to a very large network in the sense that the updates of mean and variance are also connected to all the inputs of the layers.\n",
    "\n",
    "The batch normalization layer is usually added after the linear layer and before the activation layer, and used when the batch size is large.\n",
    "\n",
    "\n",
    "For inference:\n",
    "    - we dont always have a batch of data at testing stage\n",
    "    - compute the moving average of mean and variance of the batches during training\n",
    "\n",
    "Reference:\n",
    "- how does batch normalization help optimization? https://arxiv.org/pdf/1805.11604.pdf\n",
    "\n",
    "\n",
    "Nowadays, a lot of normalization methods are proposed, such as layer normalization, instance normalization, group normalization, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Overfitting\n",
    "overfitting: get good results on training data, but bad results on test data.\n",
    "\n",
    "- parameter sharing (Fully connected -> Convolutional)\n",
    "- early stopping\n",
    "- regularization\n",
    "- dropout\n",
    "\n",
    "\n",
    "**(1). Regularization**\n",
    "\n",
    "- new loss function to be minized. For example, $L_1$ regularization, $L_2$ regularization. \n",
    "  - $L_2$ also known as weight decay. The update is like decay the weight by a factor -> close to 0\n",
    "  - $L_1$ also known as Lasso regression. The updates is like if the weight is positive, then decrease it; if the weight is negative, then increase it. -> close to 0\n",
    "\n",
    "\n",
    "**(2). Dropout**\n",
    "\n",
    "- training:\n",
    "  - randomly drop some neurons\n",
    "- testing:\n",
    "  - use all neurons, but scale the output by the dropout probability\n",
    "  - if the dropout rate at training is p%, then the weights at testing is scaled by (1-p%)\n",
    "  - dropout rate is for the whole set of neurons, not for each layer?\n",
    "\n",
    "Dropout is a kind of ensemble method. It is like training multiple sets of networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Mismatch between train and test distribution\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
