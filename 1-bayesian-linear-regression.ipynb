{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Linear Regression\n",
    "\n",
    "This is to reproduce chapter 3 of the book [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book) by Christopher M. Bishop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "import numpy as np \n",
    "from scipy.stats import norm, multivariate_normal\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from basic.basis import Polynomial\n",
    "from basic.basis import Gaussian\n",
    "from basic.linear import BayesianLinearRegression\n",
    "\n",
    "np.random.seed(111)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likilyhood Function\n",
    "\n",
    "Suppose we are given a dataset $\\mathcal{D}=\\left\\{({\\mathbf x}_n, y_n)\\vert {\\mathbf x}_n \\in \\mathbb{R}^M, y_n\\in\\mathbb{R} \\right\\}_{n=1}^N$. Where each element $y_n$ is modelled as\n",
    "\n",
    "\n",
    "$$\n",
    "    y_n\\vert {\\mathbf x}_n \\sim \\mathcal{N}({\\mathbf w}^T{\\mathbf x}, \\sigma^2)\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Distribution\n",
    "\n",
    "\n",
    "Furthermore, we we assign $\\mathbf w$ a *prior* distribution of the form ${\\mathbf w}\\sim\\mathcal{N}(\\boldsymbol\\mu_0, \\boldsymbol\\Sigma_0)$. Our goal is to find the *posterior* distribution ${\\mathbf w}\\vert \\mathcal{D}$, i.e.,\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p({\\mathbf w}\\vert \\mathcal{D}) &\\propto p({\\mathbf w})p(\\mathcal D\\vert {\\mathbf w})\\\\\n",
    "&= \\mathcal{N}({\\mathbf w}\\vert \\boldsymbol\\mu_0, \\boldsymbol\\Sigma_0) \\mathcal{N}({\\mathbf y} \\vert {\\mathbf X}{\\mathbf w}, \\sigma^2{\\mathbf I})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To find the posterior conjugate (a closed-form solution), note that\n",
    "\n",
    "$$\n",
    "    {\\mathbf z}^T{\\mathbf A}{\\mathbf z} - 2{\\mathbf z}^T{\\mathbf b} + c = ({\\mathbf z} - {\\mathbf A}^{-1}{\\mathbf b})^T{\\mathbf A}({\\mathbf z} - {\\mathbf A}^{-1}{\\mathbf b}) - {\\mathbf b}^T{\\mathbf A}^{-1}{\\mathbf b} + c\n",
    "$$\n",
    "\n",
    "Then,\n",
    "$$\n",
    "\\begin{align}\n",
    "    p({\\mathbf w}\\vert \\mathcal{D}) &\\propto \\exp\\left(-\\frac{1}{2} ({\\mathbf w} - \\boldsymbol\\mu_0)^T\\Sigma_0^{-1}({\\mathbf w} - \\boldsymbol\\mu_0) -\\frac{1}{2\\sigma^2} ({\\mathbf y} - {\\mathbf X w})^T({\\mathbf y} - {\\mathbf X w}) \\right)\\\\\n",
    "    &= \\exp\\left(-\\frac{1}{2}\\left( ({\\mathbf w} - \\boldsymbol\\mu_0)^T\\Sigma_0^{-1}({\\mathbf w} - \\boldsymbol\\mu_0) +\\frac{1}{\\sigma^2} ({\\mathbf y} - {\\mathbf X w})^T({\\mathbf y} - {\\mathbf X w})\\right) \\right)\\\\\n",
    "    &\\propto \\exp\\left(-\\frac{1}{2}\\left[{\\mathbf w}^T \\left(\\boldsymbol\\Sigma_0^{-1} + \\frac{1}{\\sigma^2}{\\mathbf X}^T{\\mathbf X}\\right) - 2{\\mathbf w}^T \\left(\\boldsymbol\\Sigma_0^{-1}\\boldsymbol\\mu_0 + \\frac{1}{\\sigma^2}{\\mathbf X}^T{\\mathbf y}\\right) \\right]\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This last expression reduces to\n",
    "\n",
    "$$\n",
    "    p({\\mathbf w}\\vert \\mathcal{D}) \\propto \\exp\\left(-\\frac{1}{2}({\\mathbf w} - {\\mathbf m}_N)^T {\\mathbf S}_N^{-1} ({\\mathbf w} - {\\mathbf m}_N)\\right)\n",
    "$$\n",
    "\n",
    "Where\n",
    "* ${\\mathbf S}_N^{-1} = \\boldsymbol\\Sigma_0^{-1} + \\frac{1}{\\sigma^2}{\\mathbf X}^T{\\mathbf X}$\n",
    "* ${\\mathbf m}_N = {\\mathbf S}_N \\left(\\boldsymbol\\Sigma_0^{-1}\\boldsymbol\\mu_0 + \\frac{1}{\\sigma^2}{\\mathbf X}^T{\\mathbf y}\\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a bayesian perspective, there does not exist *a* vector of weights $\\mathbf w$; rather, $\\mathbf w$ is a random variable that we can sample from."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is to reproduce Figure 3.7 of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_toy_data(func, sample_size, std, domain=[0, 1]):\n",
    "    x = np.linspace(domain[0], domain[1], sample_size)\n",
    "    np.random.shuffle(x)\n",
    "    t = func(x) + np.random.normal(scale=std, size=x.shape)\n",
    "    return x, t\n",
    "\n",
    "def linear(x, a0, a1):\n",
    "    return a0 + a1 * x\n",
    "\n",
    "# data generation\n",
    "a0 = -0.3\n",
    "a1 = 0.5\n",
    "std = 0.2 \n",
    "beta = 1/std**2\n",
    "x_train, y_train = create_toy_data(lambda x: linear(x, a0, a1), 20, std, [-1, 1])\n",
    "x = np.linspace(-1, 1, 100)\n",
    "\n",
    "# add polynomial features\n",
    "basis = Polynomial()\n",
    "X_train = basis.transform(x_train, degree=1)\n",
    "X = basis.transform(x, degree=1)\n",
    "\n",
    "# bayesian linear regression\n",
    "bayesian = BayesianLinearRegression()\n",
    "\n",
    "# initialize prior distribution of w\n",
    "w0, w1 = np.meshgrid(\n",
    "    np.linspace(-1, 1, 100),\n",
    "    np.linspace(-1, 1, 100))\n",
    "w = np.array([w0, w1]).transpose(1, 2, 0)\n",
    "\n",
    "alpha = 2.0 # precision of noise\n",
    "w_mean, w_var = bayesian.init_w_prior(X_train, alpha)\n",
    "\n",
    "# plot\n",
    "for begin, end in [[0,0], [0, 1], [1, 2], [2, 3], [3, 20]]:\n",
    "\n",
    "    plt.subplot(1,3,1)\n",
    "    if end > 0:\n",
    "        # calculate maximum likelihood pdf of current incomming point\n",
    "        y_obs = y_train[end-1]\n",
    "        x_obs = X_train[end-1]\n",
    "        like_mean = w.reshape(-1,2) @ x_obs.T\n",
    "        like_std = std\n",
    "        likelihood = norm.pdf(y_obs, loc=like_mean, scale=like_std)\n",
    "        likelihood = likelihood.reshape(100,100)\n",
    "        plt.contour(w0, w1, likelihood)\n",
    "        w_mean, w_var = bayesian.fit(beta, w_mean, w_var, X_train[begin: end], y_train[begin: end])\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.xlabel(\"$w_0$\")\n",
    "    plt.ylabel(\"$w_1$\")\n",
    "         \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.scatter(-0.3, 0.5, s=200, marker=\"x\")\n",
    "    plt.contour(w0, w1, multivariate_normal.pdf(w, mean=w_mean, cov=w_var))\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.title(\"prior/posterior\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.scatter(x_train[:end], y_train[:end], s=100, facecolor=\"none\", edgecolor=\"steelblue\", lw=1)\n",
    "    # sample 6 parameter pairs for plotting\n",
    "    w_samples = np.random.multivariate_normal(w_mean, w_var, size=6)\n",
    "    y_samples = X @ w_samples.T\n",
    "    plt.plot(x, y_samples, c=\"orange\")\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Predictive Distribution\n",
    "\n",
    "Most times, we are not interested in the values of parameters $\\mathbf{w}$, but instead the prediction of a new data point $\\mathbf x$. To do so, we need to integrate out the parameters $\\mathbf w$ from the posterior distribution $p({\\mathbf w}\\vert \\mathcal{D})$.\n",
    "\n",
    "Before taking the sample, the uncertainty in $\\mathbf w$ is represented by the prior distribution $p(\\mathbf w)$.\n",
    "So for new data point $\\mathbf x$, averaging over $p(\\mathbf w)$ gives the `prior predictive distribution`:\n",
    "\n",
    "$$\n",
    "p(y|\\mathbf{x}) = \\int_{\\mathbf{w}} p(y| \\mathbf{x}, \\mathbf{w}) d\\mathbf{w} = \\int_{\\mathbf{w}} p(y|\\mathbf{x}, \\mathbf{w})p(\\mathbf{w})d\\mathbf{w} \n",
    "$$\n",
    "\n",
    "After taking the sample, the uncertainty in $\\mathbf w$ is represented by the posterior distribution $p({\\mathbf w}\\vert \\mathcal{D})$. \n",
    "So the `posterior prediction distribution` is:\n",
    "\n",
    "\\begin{align}\n",
    "p(y|\\mathbf{x}, \\mathcal{D}) &= \\int_{\\mathbf{w}} p(\\textit{y}, \\mathbf{w}|\\mathbf{x}, \\mathcal{D}) d\\mathbf{w} \\\\\n",
    "                        &= \\int_{\\mathbf{w}} p(\\textit{y}|\\mathbf{x}, \\mathbf{w}, \\mathcal{D})p(\\mathbf{w}|\\mathcal{D})d\\mathbf{w}\\\\\n",
    "                        &= \\int_{\\mathbf{w}} p(\\textit{y}|\\mathbf{x}, \\mathbf{w})p(\\mathbf{w}|\\mathcal{D})d\\mathbf{w} \\leftarrow \\text{new data is independent of samples}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution of two Gaussian distributions is another Gaussian distribution. So the posterior predictive distribution is also a Gaussian distribution:\n",
    "\n",
    "$$\n",
    "p(y|\\mathbf{x}, \\mathcal{D}) = \\mathcal{N}(y|\\mathbf{x}^T\\mathbf{m}_N, \\sigma^2 + \\mathbf{x}^T\\mathbf{S}_N\\mathbf{x})\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{m}_N$ and $\\mathbf{S}_N$ are the posterior mean and covariance respectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is to reproduce Figure 3.8 - Figure 3.9 of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "samples = 25\n",
    "std = 0.25\n",
    "beta = 1/std**2\n",
    "\n",
    "x_train, y_train = create_toy_data(sinusoidal, samples, std)\n",
    "x_test = np.linspace(0, 1, 100)\n",
    "y_test = sinusoidal(x_test)\n",
    "\n",
    "feature = Gaussian()\n",
    "feature_mean, feature_var = np.linspace(0, 1, 9), 0.1\n",
    "X_train = feature.transform(x_train, feature_mean, feature_var)\n",
    "X_test = feature.transform(x_test, feature_mean, feature_var)\n",
    "\n",
    "model = BayesianLinearRegression()\n",
    "\n",
    "# initialize prior distribution of w\n",
    "w0, w1 = np.meshgrid(\n",
    "    np.linspace(-1, 1, 100),\n",
    "    np.linspace(-1, 1, 100))\n",
    "w = np.array([w0, w1]).transpose(1, 2, 0)\n",
    "\n",
    "alpha = 2.0 # precision of noise\n",
    "w_mean, w_var = model.init_w_prior(X_train, alpha)\n",
    "\n",
    "for begin, end in [[0, 1], [1, 2], [2, 4], [4, 8], [8, 25]]:\n",
    "    w_mean, w_var = model.fit(beta, w_mean, w_var, X_train[begin: end], y_train[begin: end])\n",
    "    y, y_std = model.predict(beta, w_mean, w_var, X_test)\n",
    "    \n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(x_train[:end], y_train[:end], s=100, facecolor=\"none\", edgecolor=\"steelblue\", lw=2, label=\"sample\")\n",
    "    plt.plot(x_test, y_test, label='$\\sin(2\\pi x)$')\n",
    "    plt.plot(x_test, y, label=\"mean\")\n",
    "    plt.fill_between(x_test, (y - y_std).reshape(-1), (y + y_std).reshape(-1), color=\"orange\", alpha=0.5, label='variance')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(-2, 2)\n",
    "    plt.legend()\n",
    "    plt.title('Posterior Predictive Distribution')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(x_train[:end], y_train[:end], s=100, facecolor=\"none\", edgecolor=\"steelblue\", lw=2, label=\"sample\")\n",
    "    plt.plot(x_test, y_test, label='$\\sin(2\\pi x)$')\n",
    "    # sample 6 parameter pairs for plotting\n",
    "    w_samples = np.random.multivariate_normal(w_mean, w_var, size=6)\n",
    "    y_samples = X_test @ w_samples.T\n",
    "    plt.plot(x_test, y_samples, c=\"orange\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(-2, 2)\n",
    "    plt.legend()\n",
    "    plt.title('Sample Functions')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-0.4.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
