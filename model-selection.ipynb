{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "The model selection is to choose the best model for the task. \n",
    "\n",
    "The expected prediction error (e.g., mean squared error) of a model is the sum of the bias, the variance and the irreducible error.\n",
    "\n",
    "$$E(y_0 - \\hat{f}(x_0))^2 = Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIC\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "K-fold cross validation is a method to estimate the expected prediction error.\n",
    "Assume the data set is divided into $K$ subsets, $D_1, D_2, \\dots, D_K$.\n",
    "For each subset $D_i$, we train the model on the rest of the data set $D \\setminus D_i$ and test the model on $D_i$.\n",
    "Therefore, we have $K$ set of hyperparameters for the same model and $K$ test errors.\n",
    "\n",
    "The large the $K$, the more accurate the estimate of the expected prediction error. \n",
    "With $K = N$, the so-called leave-one-out cross validation, the estimate is unbiased for the expected prediction error, but can have high variance because the $N$ training sets are so similar to one another.\n",
    "\n",
    "**Why similar training sets lead to high variance in prediction error??** \n",
    "Since the models are trained on almost identical datasets, the prediction errors for each iteration are highly correlated. As a result, the average error estimate can be sensitive to individual data points and can have a larger variance.\n",
    "\n",
    "\n",
    "Cross validation is mainly used for model selections between different models by comparing their test errors.\n",
    "Say we have a linear regression model and a neural network model, we can use cross validation to decide which model to use for the task based on the cross validation errors, which represents somehow the expected prediction errors.\n",
    "- for each model, (linear regression or neural network)\n",
    "    - perform cross-validation to get the cross validation errors by dividing the training dataset into $K$ subsets\n",
    "    - choose the model with the smallest cross validation error\n",
    "- final training\n",
    "    - train the chosen model on the whole training dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap\n",
    "\n",
    "Bootstrap is a method to estimate the bias and variance of a model.\n",
    "Bootstrap method resamples the training dataset with replacement to generate $B$ bootstrap datasets.\n",
    "The resampled datasets has the same size as the original dataset, but some data points are missing and some are duplicated.\n",
    "\n",
    "- for each bootstrap dataset\n",
    "    - train the model on the bootstrap dataset\n",
    "- evaluate the model on the original dataset\n",
    "    - for each observation in the original dataset\n",
    "        - keep track of the predictions from the bootstrap models not containing that observation\n",
    "        -  average the prediction errors\n",
    "\n",
    "\n",
    "Some properties:\n",
    "- bootstrap is a computer implementation of nonparametric or parametric maximum likelihood estimation. It allows to compute maximum likelihood estimates of standard errors and other quantities in settings where no maximum likelihood formulas are available.\n",
    "- bootstrap mean is approximately a posterior average, from Bayesian perspective.\n",
    "  - Since the posterior mean (not mode) minimizes squared-error loss, it is not surprising that bagging can often reduce mean squared-error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "1. How can the cross-validation and bootstrap methods help balance the bias and variance?\n",
    "    - cross validation can estimate expected prediction error\n",
    "        - MSE as a measure of expected prediction error\n",
    "        - MSE = bias + variance + irreducible error\n",
    "        - bias and variance are inversely related\n",
    "    - model selected by cross validation is the one with the smallest expected prediction error\n",
    "    - the model with the smallest expected prediction error is the one with the smallest bias and variance\n",
    "2. What is the difference between cross validation and ensemble learning?\n",
    "    - cross-validation and boostrap are just resampling methods, which are used to estimate the variance and bias of a model, to quantify the uncerntainty of the model, and to select the best model in terms of generalization error.\n",
    "    - ensemble learning is a method to combine multiple models to improve the performance of a single model.\n",
    "        - bagging: bootstrap aggregation. \n",
    "            - train multiple models on different bootstrap datasets and average the predictions\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
