{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning System Design\n",
    "\n",
    "## Prioritizing What to Work On\n",
    "How to improve algorithm performance?\n",
    "\n",
    "Spam classifier example:\n",
    "\n",
    "- supervised learning (classification)\n",
    "    - features: (x) = features of email -> e.g., indicative words\n",
    "    - labels: (y) = spam (1) or not spam (0)\n",
    "\n",
    "How to spend time to improve performance of spam classifier?\n",
    "- collect lots of data\n",
    "    - get data, for example, honeypot project\n",
    "- develop sophisticated features (x)\n",
    "    - e.g., using email header data in spam emails\n",
    "    - e.g., using email body. Features about punctuation, etc.\n",
    "- develop algorithms to process your input in different ways \n",
    "    - e.g., detect misspellings (x), such as m0rtgage, med1cine, w4tches."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Recommended approach:\n",
    "- start with a simple algorithm that you can implement quickly. implement it and test it on your cross-validation data.\n",
    "- plot learning curves to decide if more data, more features, etc, are likely to help.\n",
    "- error analysis: manually examine the examples (in cross validation set) that your algorithm made errors on. see if you spot any systematic trend in what type of examples it is making errors on.\n",
    "    - e.g., mislabelled data -> correct labels\n",
    "    - e.g., misclassifed -> any new feature would help to reduce?\n",
    "- error metrics: numerical evaluation of error, e.g., precision/recall, F1 score, etc.\n",
    "    - skewed data: e.g., 100 emails, 99 non-spam, 1 spam. 99% accuracy, but not useful.\n",
    "        - precision/recall: \n",
    "            - precision = true positive / predicted positives = true positives / (true positives + false positives)\n",
    "                - precision: of all emails predicted as spam, what fraction is actually spam?\n",
    "            - recall: true positives / actual positives = true positives / (true positives + false negatives)\n",
    "                - recall: of all the emails that are actually spam, what fraction did we correctly detect as spam?\n",
    "            - tradeoff between precision and recall\n",
    "        - F1 score -> how to decide which algorithm is better? e.g., algorithm 1 with (P,R) = (0.5, 0.4), algorithm 2 with (P,R) = (0.7, 0.1)\n",
    "            - F1 score = 2 * (P * R) / (P + R)\n",
    "                - simple average of precision and recall will not work as the difference after average might be very small for different algorithms\n",
    "            - F1 score algorithm 1 = 0.44, F1 score algorithm 2 = 0.18\n",
    "            - F1 score is the harmonic mean of precision and recall\n",
    "            - F1 score is a single number summary of precision/recall tradeoff\n",
    "            - F1 score is a way to compare two classifiers\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
