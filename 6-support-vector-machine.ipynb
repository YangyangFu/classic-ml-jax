{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jaxopt import BoxOSQP\n",
    "\n",
    "from basic.kernel.kernel import Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinarySVC():\n",
    "    \"\"\" Support Vector Machine Binary Classifier\n",
    "     \n",
    "    \"\"\"\n",
    "    kernel = Linear()\n",
    "    kernel_params = None\n",
    "    C = 2.0\n",
    "    tolerance = 1e-6\n",
    "\n",
    "    def fit(self, X, t):\n",
    "        \"\"\"fit model to data\n",
    "        \n",
    "        Solve the following dual optimization problem:\n",
    "            $$ \\min_{\\alpha} \\frac{1}{2}(\\alpha t)^T K (\\alpha t) - (\\alpha t)^T t $$ \n",
    "            \n",
    "            subject to:\n",
    "            \n",
    "            $$ 0 \\leq \\alpha_i \\leq C $$\n",
    "            $$ \\sum_{i=1}^{N} \\alpha_i t_i = 0 $$\n",
    "            \n",
    "            reformulation by substituting $ \\beta = \\alpha t $:\n",
    "            $$ \\min_{\\beta} \\frac{1}{2} \\beta^T K \\beta - \\beta^T 1 $$\n",
    "            \n",
    "        Args:\n",
    "            X (jnp.array, (N, D)): input data\n",
    "            t (jnp.array, (N,)): target data\n",
    "            kernel (function): kernel function\n",
    "            kernel_params (dict): kernel parameters\n",
    "            C (float): regularization parameter\n",
    "            \n",
    "        Returns:\n",
    "            dict: dictionary of parameters\n",
    "        \"\"\"\n",
    "        \n",
    "        def matvec_Q(X, beta):\n",
    "            # the objective implementation in OSQP is 0.5*x^T * matvec_Q(P,x)\n",
    "            # this returns Kbeta = X X^T beta\n",
    "            # because OSQP assume 0.5*x^T * matvec_Q(P,x) in the objective\n",
    "            # return shape: (N,)\n",
    "            \n",
    "            Gram = self.kernel.kernel(self.kernel_params, X, X)\n",
    "            return Gram @ beta\n",
    "\n",
    "        def matvec_A(_, beta):\n",
    "            return beta, jnp.sum(beta)\n",
    "        \n",
    "        # l, u must have same shape as matvec_A's output.\n",
    "        l = -jax.nn.relu(-t * self.C), 0.\n",
    "        u =  jax.nn.relu( t * self.C), 0.\n",
    "        \n",
    "        # formulate and solve quadratic programming problem\n",
    "        hyper_params = dict(params_obj=(X, -t), params_eq=None, params_ineq=(l, u))\n",
    "        osqp = BoxOSQP(matvec_Q=matvec_Q, matvec_A=matvec_A, tol=self.tolerance)\n",
    "        params, _ = osqp.run(init_params=None, **hyper_params)\n",
    "        beta = params.primal[0]\n",
    "\n",
    "        # get support vectors\n",
    "        sv = self.get_support_vectors(beta)\n",
    "        \n",
    "        return beta, sv\n",
    "\n",
    "    def get_support_vectors(self, beta):\n",
    "        # beta is signed \n",
    "        # beta = 0 means the Langrange multiplier is 0, which means the corresponding sample does not contribute to the sum in the objective function.\n",
    "        # beta ~= 0 means the samples are support vectors\n",
    "        \n",
    "        is_sc = jnp.abs(beta) > self.tolerance\n",
    "\n",
    "        return jnp.where(is_sc)[0]\n",
    "    \n",
    "    def _accuracy(self):\n",
    "        \"\"\"get accuracy of model:\n",
    "            if 0 < abs(beta) < C, then epsilon = 0, then the sample is on the margin\n",
    "            if abs(beta) = C, then the sample can lie inside the margin and can either be correctly classied if epsilon <=1 or misclassified if epsilon > 1\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X_test, X_train, y_train, beta, sv):\n",
    "        \n",
    "        \"\"\"solving primal problem gives w and b\n",
    "            From Eq. (7.29) and (7.37) in Bishop's book:\n",
    "            $$ w = \\sum_{i=1}^{N} \\alpha_i t_i x_i = (\\beta^T x)^T = x^T \\beta $$\n",
    "            $$ wx = w^Tx^T = \\beta^T x x^T = \\beta^T K$$\n",
    "            \n",
    "        \"\"\"\n",
    "        # get wx\n",
    "        Gram = self.kernel.kernel(self.kernel_params, X_train[sv], X_test)\n",
    "        wx = beta[sv].T @ Gram \n",
    "        \n",
    "        # get b\n",
    "        # get indice of support vectors on the margin: 0 < abs(beta) < C\n",
    "        M_mask = jnp.abs(beta[sv]) < self.C-self.tolerance\n",
    "        Gram_S = self.kernel.kernel(self.kernel_params, X_train[sv], X_train[sv]) # (S,S)\n",
    "\n",
    "        # define some jittable functions\n",
    "        def set_nonmargin_to_zero(x, M):\n",
    "            return jnp.where(M, x, 0)\n",
    "        \n",
    "        def get_nonzero_mean(x):\n",
    "            return jnp.mean(x, where = x != 0)\n",
    "                    \n",
    "        bv = set_nonmargin_to_zero(y_train[sv] - Gram_S @ beta[sv], M_mask)\n",
    "        b = get_nonzero_mean(bv)\n",
    "\n",
    "        # This version is not jittable, and seems slightly different for the final b\n",
    "        # b1 \n",
    "        #Gram_M = kernel.kernel(kernel_params, X_train[sv][M_mask], X_train[sv]) # (M, S)\n",
    "        #bv1 = y_train[sv][M_mask] - Gram_M @ beta[sv]\n",
    "        #b1 = jnp.mean(bv1)\n",
    "        #print(bv1.shape, b1, bv1)\n",
    "        #print(b)\n",
    "        # retur signs of wx + b: 1 or -1\n",
    "        return jnp.sign(wx + b)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BinarySVC' object has no attribute 'tol'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m linear_kernel \u001b[39m=\u001b[39m Linear()\n\u001b[1;32m     19\u001b[0m svc \u001b[39m=\u001b[39m BinarySVC()\n\u001b[0;32m---> 20\u001b[0m beta, sv \u001b[39m=\u001b[39m svc\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[1;32m     22\u001b[0m \u001b[39m# predict\u001b[39;00m\n\u001b[1;32m     23\u001b[0m svc_predict \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mjit(svc\u001b[39m.\u001b[39mpredict)\n",
      "Cell \u001b[0;32mIn[126], line 53\u001b[0m, in \u001b[0;36mBinarySVC.fit\u001b[0;34m(self, X, t)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m# formulate and solve quadratic programming problem\u001b[39;00m\n\u001b[1;32m     52\u001b[0m hyper_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(params_obj\u001b[39m=\u001b[39m(X, \u001b[39m-\u001b[39mt), params_eq\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, params_ineq\u001b[39m=\u001b[39m(l, u))\n\u001b[0;32m---> 53\u001b[0m osqp \u001b[39m=\u001b[39m BoxOSQP(matvec_Q\u001b[39m=\u001b[39mmatvec_Q, matvec_A\u001b[39m=\u001b[39mmatvec_A, tol\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol)\n\u001b[1;32m     54\u001b[0m params, _ \u001b[39m=\u001b[39m osqp\u001b[39m.\u001b[39mrun(init_params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhyper_params)\n\u001b[1;32m     55\u001b[0m beta \u001b[39m=\u001b[39m params\u001b[39m.\u001b[39mprimal[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BinarySVC' object has no attribute 'tol'"
     ]
    }
   ],
   "source": [
    "lam = 0.5\n",
    "tol = 1e-06\n",
    "num_samples = 30\n",
    "num_features = 5\n",
    "verbose = False\n",
    "\n",
    "# Prepare data.\n",
    "X, y = datasets.make_classification(n_samples=num_samples, n_features=num_features,\n",
    "                                n_classes=2,\n",
    "                                random_state=0)\n",
    "X = preprocessing.Normalizer().fit_transform(X)\n",
    "y = jnp.array(y * 2. - 1)  # Transform labels from {0, 1} to {-1., 1.}.\n",
    "\n",
    "C = 1./ lam\n",
    "\n",
    "# Compare the obtained dual coefficients.\n",
    "# kernels \n",
    "linear_kernel = Linear()\n",
    "svc = BinarySVC()\n",
    "beta, sv = svc.fit(X, y)\n",
    "\n",
    "# predict\n",
    "svc_predict = jax.jit(svc.predict)\n",
    "#svc_predict = svc.predict\n",
    "y_predict = svc_predict(X_test=X, \n",
    "                        X_train=X, \n",
    "                        y_train=y, \n",
    "                        beta=beta, \n",
    "                        sv=sv)\n",
    "\n",
    "print(jnp.abs(y_predict - y).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve SVM with OSQP: \n",
      "Beta: [ 2.8452001e-07 -1.9592164e-07  7.1804309e-01 -3.1301258e-07\n",
      "  7.4410588e-07  7.0679100e-08 -4.3022047e-07 -2.0713337e-07\n",
      "  5.9224438e-02  2.0000005e+00  2.0000005e+00 -1.6404691e-07\n",
      " -1.9999998e+00  2.0000000e+00 -3.8241390e-07 -1.9999998e+00\n",
      " -1.5248646e-07 -7.4367165e-07 -7.7726805e-01 -3.8845691e-08\n",
      "  3.8410909e-07 -2.0431916e-08  2.2927431e-08  5.0132905e-07\n",
      " -3.4100701e-08 -4.6190539e-07 -2.0000002e+00 -1.7885380e-07\n",
      " -1.2326537e-07  5.6264736e-07]\n",
      "Support vector indices: [ 2  8  9 10 12 13 15 18 26]\n",
      "\n",
      "Solve SVM with Projected Gradient: \n",
      "Beta: [ 0.          0.          0.718046    0.          0.          0.\n",
      "  0.          0.          0.05922639  2.          2.          0.\n",
      " -2.          2.          0.         -2.          0.          0.\n",
      " -0.77726483  0.          0.          0.          0.          0.\n",
      "  0.          0.         -2.          0.          0.          0.        ]\n",
      "Support vector indices: [ 2  8  9 10 12 13 15 18 26]\n",
      "\n",
      "Solve SVM with sklearn.svm.SVC: \n",
      "[0.28367585]\n",
      "Beta: [ 0.          0.          0.71804308  0.          0.          0.\n",
      "  0.          0.          0.05922482  2.          2.          0.\n",
      " -2.          2.          0.         -2.          0.          0.\n",
      " -0.77726791  0.          0.          0.          0.          0.\n",
      "  0.          0.         -2.          0.          0.          0.        ]\n",
      "Support vector indices: [ 2  8  9 10 12 13 15 18 26]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from absl import app\n",
    "#from absl import flags\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jaxopt import projection\n",
    "from jaxopt import ProjectedGradient\n",
    "from jaxopt import BoxOSQP\n",
    "\n",
    "import numpy as onp\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "\n",
    "tol = 1e-06 \n",
    "verbose = False\n",
    "    \n",
    "def binary_kernel_svm_skl(X, y, C):\n",
    "    print(f\"Solve SVM with sklearn.svm.SVC: \")\n",
    "    K = jnp.dot(X, X.T)\n",
    "    svc = svm.SVC(kernel=\"precomputed\", C=C, tol=tol).fit(K, y)\n",
    "    dual_coef = onp.zeros(K.shape[0])\n",
    "    dual_coef[svc.support_] = svc.dual_coef_[0]\n",
    "    print(svc.intercept_)\n",
    "    return dual_coef\n",
    "\n",
    "\n",
    "def binary_kernel_svm_pg(X, y, C):\n",
    "    print(f\"Solve SVM with Projected Gradient: \")\n",
    "\n",
    "    def objective_fun(beta, X, y):\n",
    "        \"\"\"Dual objective of binary kernel SVMs with intercept.\"\"\"\n",
    "        # The dual objective is:\n",
    "        # fun(beta) = 0.5 beta^T K beta - beta^T y\n",
    "        # subject to\n",
    "        # sum(beta) = 0\n",
    "        # 0 <= beta_i <= C if y_i = 1\n",
    "        # -C <= beta_i <= 0 if y_i = -1\n",
    "        # where C = 1.0 / lam\n",
    "        # and K = X X^T\n",
    "        Kbeta = jnp.dot(X, jnp.dot(X.T, beta))\n",
    "\n",
    "        return 0.5 * jnp.dot(beta, Kbeta) - jnp.dot(beta, y)\n",
    "\n",
    "    # Define projection operator.\n",
    "    w = jnp.ones(X.shape[0])\n",
    "\n",
    "    def proj(beta, C):\n",
    "        box_lower = jnp.where(y == 1, 0, -C)\n",
    "        box_upper = jnp.where(y == 1, C, 0)\n",
    "        proj_params = (box_lower, box_upper, w, 0.0)\n",
    "        return projection.projection_box_section(beta, proj_params)\n",
    "\n",
    "    # Run solver.\n",
    "    beta_init = jnp.ones(X.shape[0])\n",
    "    solver = ProjectedGradient(fun=objective_fun,\n",
    "                                projection=proj,\n",
    "                                tol=tol, maxiter=500, verbose=verbose)\n",
    "    beta_fit = solver.run(beta_init, hyperparams_proj=C, X=X, y=y).params\n",
    "\n",
    "    return beta_fit\n",
    "\n",
    "\n",
    "def binary_kernel_svm_osqp(X, y, C):\n",
    "    # The dual objective is:\n",
    "    # fun(beta) = 0.5 beta^T K beta - beta^T y\n",
    "    # subject to\n",
    "    # sum(beta) = 0\n",
    "    # 0 <= beta_i <= C if y_i = 1\n",
    "    # -C <= beta_i <= 0 if y_i = -1\n",
    "    # where C = 1.0 / lam\n",
    "\n",
    "    print(f\"Solve SVM with OSQP: \")\n",
    "\n",
    "    def matvec_Q(X, beta):\n",
    "        # the objective implementation in OSQP is 0.5*x^T * matvec_Q(P,x)\n",
    "        # this returns Kbeta = X X^T beta\n",
    "        # because OSQP assume 0.5*x^T * matvec_Q(P,x) in the objective\n",
    "        return jnp.dot(X, jnp.dot(X.T,  beta))\n",
    "\n",
    "    # There qre two types of constraints:\n",
    "    #   0 <= y_i * beta_i <= C     (1)\n",
    "    # and:\n",
    "    #   sum(beta) = 0              (2)\n",
    "    # The first one involves the identity matrix over the betas.\n",
    "    # The second one involves their sum (i.e dot product with vector full of 1).\n",
    "    # We take advantage of matvecs to avoid materializing A in memory.\n",
    "    # We return a tuple whose entries correspond each type of constraint.\n",
    "    def matvec_A(_, beta):\n",
    "        return beta, jnp.sum(beta)\n",
    "\n",
    "    # l, u must have same shape than matvec_A's output.\n",
    "    l = -jax.nn.relu(-y * C), 0.\n",
    "    u =  jax.nn.relu( y * C), 0.\n",
    "\n",
    "    hyper_params = dict(params_obj=(X, -y), params_eq=None, params_ineq=(l, u))\n",
    "    osqp = BoxOSQP(matvec_Q=matvec_Q, matvec_A=matvec_A, tol=tol)\n",
    "    params, _ = osqp.run(init_params=None, **hyper_params)\n",
    "    beta = params.primal[0]\n",
    "\n",
    "    return beta\n",
    "\n",
    "\n",
    "def print_svm_result(beta, threshold=1e-4):\n",
    "    # Here the vector `beta` of coefficients is signed:\n",
    "    # its sign depends of the true label of the corresponding example.\n",
    "    # Hence we use jnp.abs() to detect support vectors.\n",
    "    is_support_vectors = jnp.abs(beta) > threshold\n",
    "    print(f\"Beta: {beta}\")\n",
    "    print(f\"Support vector indices: {onp.where(is_support_vectors)[0]}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    lam = 0.5\n",
    "    num_samples = 30\n",
    "    num_features = 5\n",
    "\n",
    "\n",
    "    # Prepare data.\n",
    "    X, y = datasets.make_classification(n_samples=num_samples, n_features=num_features,\n",
    "                                        n_classes=2,\n",
    "                                        random_state=0)\n",
    "    X = preprocessing.Normalizer().fit_transform(X)\n",
    "    y = jnp.array(y * 2. - 1)  # Transform labels from {0, 1} to {-1., 1.}.\n",
    "\n",
    "    C = 1./ lam\n",
    "\n",
    "    beta_fit_osqp = binary_kernel_svm_osqp(X, y, C)\n",
    "    print_svm_result(beta_fit_osqp)\n",
    "\n",
    "    beta_fit_pg = binary_kernel_svm_pg(X, y, C)\n",
    "    print_svm_result(beta_fit_pg)\n",
    "\n",
    "    beta_fit_skl = binary_kernel_svm_skl(X, y, C)\n",
    "    print_svm_result(beta_fit_skl)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-0.4.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
