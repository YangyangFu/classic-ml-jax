{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jaxopt import BoxOSQP\n",
    "\n",
    "class BinarySVC():\n",
    "    \"\"\" Support Vector Machine Binary Classifier\n",
    "     \n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, t, C, kernel=None, kernel_params=None, tol=1e-6):\n",
    "        \"\"\"fit model to data\n",
    "        \n",
    "        Solve the following dual optimization problem:\n",
    "            $$ \\min_{\\alpha} \\frac{1}{2}(\\alpha t)^T K (\\alpha t) - (\\alpha t)^T t $$ \n",
    "            \n",
    "            subject to:\n",
    "            \n",
    "            $$ 0 \\leq \\alpha_i \\leq C $$\n",
    "            $$ \\sum_{i=1}^{N} \\alpha_i t_i = 0 $$\n",
    "            \n",
    "            reformulation by substituting $ \\beta = \\alpha t $:\n",
    "            $$ \\min_{\\beta} \\frac{1}{2} \\beta^T K \\beta - \\beta^T 1 $$\n",
    "            \n",
    "        Args:\n",
    "            X (jnp.array, (N, D)): input data\n",
    "            t (jnp.array, (N,)): target data\n",
    "            kernel (function): kernel function\n",
    "            kernel_params (dict): kernel parameters\n",
    "            C (float): regularization parameter\n",
    "            \n",
    "        Returns:\n",
    "            dict: dictionary of parameters\n",
    "        \"\"\"\n",
    "        \n",
    "        def matvec_Q(X, beta):\n",
    "            # the objective implementation in OSQP is 0.5*x^T * matvec_Q(P,x)\n",
    "            # this returns Kbeta = X X^T beta\n",
    "            # because OSQP assume 0.5*x^T * matvec_Q(P,x) in the objective\n",
    "            # return shape: (N,)\n",
    "            \n",
    "            Gram = kernel.kernel(kernel_params, X, X)\n",
    "            return Gram @ beta\n",
    "\n",
    "        def matvec_A(_, beta):\n",
    "            return beta, jnp.sum(beta)\n",
    "        \n",
    "        # l, u must have same shape as matvec_A's output.\n",
    "        l = -jax.nn.relu(-t * C), 0.\n",
    "        u =  jax.nn.relu( t * C), 0.\n",
    "        \n",
    "        # formulate and solve quadratic programming problem\n",
    "        hyper_params = dict(params_obj=(X, -t), params_eq=None, params_ineq=(l, u))\n",
    "        osqp = BoxOSQP(matvec_Q=matvec_Q, matvec_A=matvec_A, tol=tol)\n",
    "        params, _ = osqp.run(init_params=None, **hyper_params)\n",
    "        beta = params.primal[0]\n",
    "\n",
    "        # get support vectors\n",
    "        sv = self.get_support_vectors(beta)\n",
    "        \n",
    "        return beta, sv\n",
    "\n",
    "    def get_support_vectors(self, beta, tolerance = 1e-6):\n",
    "        # beta is signed \n",
    "        # beta = 0 means the Langrange multiplier is 0, which means the corresponding sample does not contribute to the sum in the objective function.\n",
    "        # beta ~= 0 means the samples are support vectors\n",
    "        \n",
    "        is_sc = jnp.abs(beta) > tolerance\n",
    "\n",
    "        return jnp.where(is_sc)[0]\n",
    "    \n",
    "    def predict(X, beta, kernel, kernel_params):\n",
    "        \n",
    "        \"\"\"solving primal problem gives w and b\n",
    "            From Eq. (7.29) and (7.37) in Bishop's book:\n",
    "            $$ w = \\sum_{i=1}^{N} \\alpha_i t_i x_i = (\\beta^T x)^T = x^T \\beta $$\n",
    "            $$ wx = w^Tx^T = \\beta^T x x^T = \\beta^T K$$\n",
    "            \n",
    "            $$ b = $$\n",
    "        \"\"\"\n",
    "        Gram = kernel.kernel(kernel_params, X, X)\n",
    "        wx = beta.T @ Gram\n",
    "        \n",
    "        return jnp.sign(wx)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve SVM with sklearn.svm.SVC: \n",
      "Beta: [ 0.          0.          0.71804308  0.          0.          0.\n",
      "  0.          0.          0.05922482  2.          2.          0.\n",
      " -2.          2.          0.         -2.          0.          0.\n",
      " -0.77726791  0.          0.          0.          0.          0.\n",
      "  0.          0.         -2.          0.          0.          0.        ]\n",
      "Support vector indices: [ 2  8  9 10 12 13 15 18 26]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from absl import app\n",
    "#from absl import flags\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jaxopt import projection\n",
    "from jaxopt import ProjectedGradient\n",
    "from jaxopt import BoxOSQP\n",
    "\n",
    "import numpy as onp\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "\n",
    "tol = 1e-06 \n",
    "verbose = False\n",
    "    \n",
    "def binary_kernel_svm_skl(X, y, C):\n",
    "    print(f\"Solve SVM with sklearn.svm.SVC: \")\n",
    "    K = jnp.dot(X, X.T)\n",
    "    svc = svm.SVC(kernel=\"precomputed\", C=C, tol=tol).fit(K, y)\n",
    "    dual_coef = onp.zeros(K.shape[0])\n",
    "    dual_coef[svc.support_] = svc.dual_coef_[0]\n",
    "    return dual_coef\n",
    "\n",
    "\n",
    "def binary_kernel_svm_pg(X, y, C):\n",
    "    print(f\"Solve SVM with Projected Gradient: \")\n",
    "\n",
    "    def objective_fun(beta, X, y):\n",
    "        \"\"\"Dual objective of binary kernel SVMs with intercept.\"\"\"\n",
    "        # The dual objective is:\n",
    "        # fun(beta) = 0.5 beta^T K beta - beta^T y\n",
    "        # subject to\n",
    "        # sum(beta) = 0\n",
    "        # 0 <= beta_i <= C if y_i = 1\n",
    "        # -C <= beta_i <= 0 if y_i = -1\n",
    "        # where C = 1.0 / lam\n",
    "        # and K = X X^T\n",
    "        Kbeta = jnp.dot(X, jnp.dot(X.T, beta))\n",
    "\n",
    "        return 0.5 * jnp.dot(beta, Kbeta) - jnp.dot(beta, y)\n",
    "\n",
    "    # Define projection operator.\n",
    "    w = jnp.ones(X.shape[0])\n",
    "\n",
    "    def proj(beta, C):\n",
    "        box_lower = jnp.where(y == 1, 0, -C)\n",
    "        box_upper = jnp.where(y == 1, C, 0)\n",
    "        proj_params = (box_lower, box_upper, w, 0.0)\n",
    "        return projection.projection_box_section(beta, proj_params)\n",
    "\n",
    "    # Run solver.\n",
    "    beta_init = jnp.ones(X.shape[0])\n",
    "    solver = ProjectedGradient(fun=objective_fun,\n",
    "                                projection=proj,\n",
    "                                tol=tol, maxiter=500, verbose=verbose)\n",
    "    beta_fit = solver.run(beta_init, hyperparams_proj=C, X=X, y=y).params\n",
    "\n",
    "    return beta_fit\n",
    "\n",
    "\n",
    "def binary_kernel_svm_osqp(X, y, C):\n",
    "    # The dual objective is:\n",
    "    # fun(beta) = 0.5 beta^T K beta - beta^T y\n",
    "    # subject to\n",
    "    # sum(beta) = 0\n",
    "    # 0 <= beta_i <= C if y_i = 1\n",
    "    # -C <= beta_i <= 0 if y_i = -1\n",
    "    # where C = 1.0 / lam\n",
    "\n",
    "    print(f\"Solve SVM with OSQP: \")\n",
    "\n",
    "    def matvec_Q(X, beta):\n",
    "        # the objective implementation in OSQP is 0.5*x^T * matvec_Q(P,x)\n",
    "        # this returns Kbeta = X X^T beta\n",
    "        # because OSQP assume 0.5*x^T * matvec_Q(P,x) in the objective\n",
    "        return jnp.dot(X, jnp.dot(X.T,  beta))\n",
    "\n",
    "    # There qre two types of constraints:\n",
    "    #   0 <= y_i * beta_i <= C     (1)\n",
    "    # and:\n",
    "    #   sum(beta) = 0              (2)\n",
    "    # The first one involves the identity matrix over the betas.\n",
    "    # The second one involves their sum (i.e dot product with vector full of 1).\n",
    "    # We take advantage of matvecs to avoid materializing A in memory.\n",
    "    # We return a tuple whose entries correspond each type of constraint.\n",
    "    def matvec_A(_, beta):\n",
    "        return beta, jnp.sum(beta)\n",
    "\n",
    "    # l, u must have same shape than matvec_A's output.\n",
    "    l = -jax.nn.relu(-y * C), 0.\n",
    "    u =  jax.nn.relu( y * C), 0.\n",
    "\n",
    "    hyper_params = dict(params_obj=(X, -y), params_eq=None, params_ineq=(l, u))\n",
    "    osqp = BoxOSQP(matvec_Q=matvec_Q, matvec_A=matvec_A, tol=tol)\n",
    "    params, _ = osqp.run(init_params=None, **hyper_params)\n",
    "    beta = params.primal[0]\n",
    "\n",
    "    return beta\n",
    "\n",
    "\n",
    "def print_svm_result(beta, threshold=1e-4):\n",
    "    # Here the vector `beta` of coefficients is signed:\n",
    "    # its sign depends of the true label of the corresponding example.\n",
    "    # Hence we use jnp.abs() to detect support vectors.\n",
    "    is_support_vectors = jnp.abs(beta) > threshold\n",
    "    print(f\"Beta: {beta}\")\n",
    "    print(f\"Support vector indices: {onp.where(is_support_vectors)[0]}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    lam = 0.5\n",
    "    num_samples = 30\n",
    "    num_features = 5\n",
    "\n",
    "\n",
    "    # Prepare data.\n",
    "    X, y = datasets.make_classification(n_samples=num_samples, n_features=num_features,\n",
    "                                        n_classes=2,\n",
    "                                        random_state=0)\n",
    "    X = preprocessing.Normalizer().fit_transform(X)\n",
    "    y = jnp.array(y * 2. - 1)  # Transform labels from {0, 1} to {-1., 1.}.\n",
    "\n",
    "    C = 1./ lam\n",
    "\n",
    "    beta_fit_osqp = binary_kernel_svm_osqp(X, y, C)\n",
    "    print_svm_result(beta_fit_osqp)\n",
    "\n",
    "    beta_fit_pg = binary_kernel_svm_pg(X, y, C)\n",
    "    print_svm_result(beta_fit_pg)\n",
    "\n",
    "    beta_fit_skl = binary_kernel_svm_skl(X, y, C)\n",
    "    print_svm_result(beta_fit_skl)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from basic.kernel.kernel import Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.2570302e-07 -7.0080171e-07  7.1804303e-01 -7.3801027e-07\n",
      " -1.5326397e-07 -3.7119069e-08  2.7381390e-07  7.5128497e-08\n",
      "  5.9225108e-02  2.0000007e+00  1.9999995e+00  1.6718060e-07\n",
      " -1.9999995e+00  2.0000010e+00  1.5628810e-08 -1.9999996e+00\n",
      "  4.2407018e-07 -4.3545480e-08 -7.7726763e-01  4.7609350e-07\n",
      " -1.7563620e-07  1.6845306e-07 -4.5571153e-07 -3.1937725e-07\n",
      "  7.7016722e-08 -6.1330627e-07 -2.0000007e+00  3.3309587e-07\n",
      "  1.7144387e-07 -3.4182463e-07]\n",
      "the support vectors are: \n",
      "[ 2  8  9 10 12 13 15 18 26]\n"
     ]
    }
   ],
   "source": [
    "lam = 0.5\n",
    "tol = 1e-06\n",
    "num_samples = 30\n",
    "num_features = 5\n",
    "verbose = False\n",
    "\n",
    "# Prepare data.\n",
    "X, y = datasets.make_classification(n_samples=num_samples, n_features=num_features,\n",
    "                                n_classes=2,\n",
    "                                random_state=0)\n",
    "X = preprocessing.Normalizer().fit_transform(X)\n",
    "y = jnp.array(y * 2. - 1)  # Transform labels from {0, 1} to {-1., 1.}.\n",
    "\n",
    "C = 1./ lam\n",
    "\n",
    "# Compare the obtained dual coefficients.\n",
    "svc = BinarySVC()\n",
    "\n",
    "# kernels \n",
    "linear_kernel = Linear()\n",
    "beta, sv = svc.fit(X, y, C, kernel=linear_kernel)\n",
    "print(beta)\n",
    "\n",
    "print(\"The support vectors are: \")\n",
    "print(sv)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-0.4.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
