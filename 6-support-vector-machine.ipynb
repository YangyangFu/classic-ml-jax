{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "- https://www.youtube.com/watch?v=QSEPStBgwRQ\n",
    "- pattern recognition and machine learning, chapter 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between SVM and Logistic Regression\n",
    "- major difference is that SVM uses hinge loss while logistic regression uses cross-entropy loss."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Problem\n",
    "\n",
    "0. Data\n",
    "\n",
    "    x: input, $x \\in \\mathbb{R}^D$\n",
    "\n",
    "    t: target, $t \\in \\{-1, 1\\}$\n",
    "\n",
    "1. Model\n",
    "\n",
    "$$y(x) = w^T x + b$$\n",
    "\n",
    "$$\n",
    "g(x) = \\begin{cases}\n",
    "1 & \\text{if } y(x) \\geq 0 \\\\\n",
    "-1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "2. Loss Function\n",
    "\n",
    "$$ \n",
    "L(y) = \\sum_{n=1}^N \\delta (g(x_n) \\neq t_n) = \\sum_{n=1}^N l(y(x_n), t_n)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge Loss and Cross-Entropy Loss\n",
    "\n",
    "- ideal loss function: 0-1 loss\n",
    "    $$\n",
    "    l = \\begin{cases}\n",
    "    0 & \\text{if } t_ny(x_n) \\geq 0 \\\\\n",
    "    1 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Formulation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Trick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jaxopt import BoxOSQP\n",
    "\n",
    "from basic.kernel.kernel import Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinarySVC():\n",
    "    \"\"\" Support Vector Machine Binary Classifier\n",
    "     \n",
    "    \"\"\"\n",
    "    kernel = Linear()\n",
    "    kernel_params = None\n",
    "    C = 2.0\n",
    "    tolerance = 1e-6\n",
    "\n",
    "    def fit(self, X, t):\n",
    "        \"\"\"fit model to data\n",
    "        \n",
    "        Solve the following dual optimization problem:\n",
    "            $$ \\min_{\\alpha} \\frac{1}{2}(\\alpha t)^T K (\\alpha t) - (\\alpha t)^T t $$ \n",
    "            \n",
    "            subject to:\n",
    "            \n",
    "            $$ 0 \\leq \\alpha_i \\leq C $$\n",
    "            $$ \\sum_{i=1}^{N} \\alpha_i t_i = 0 $$\n",
    "            \n",
    "            reformulation by substituting $ \\beta = \\alpha t $:\n",
    "            $$ \\min_{\\beta} \\frac{1}{2} \\beta^T K \\beta - \\beta^T 1 $$\n",
    "            \n",
    "        Args:\n",
    "            X (jnp.array, (N, D)): input data\n",
    "            t (jnp.array, (N,)): target data\n",
    "            kernel (function): kernel function\n",
    "            kernel_params (dict): kernel parameters\n",
    "            C (float): regularization parameter\n",
    "            \n",
    "        Returns:\n",
    "            dict: dictionary of parameters\n",
    "        \"\"\"\n",
    "        \n",
    "        def matvec_Q(X, beta):\n",
    "            # the objective implementation in OSQP is 0.5*x^T * matvec_Q(P,x)\n",
    "            # this returns Kbeta = X X^T beta\n",
    "            # because OSQP assume 0.5*x^T * matvec_Q(P,x) in the objective\n",
    "            # return shape: (N,)\n",
    "            \n",
    "            Gram = self.kernel.kernel(self.kernel_params, X, X)\n",
    "            return Gram @ beta\n",
    "\n",
    "        def matvec_A(_, beta):\n",
    "            return beta, jnp.sum(beta)\n",
    "        \n",
    "        # l, u must have same shape as matvec_A's output.\n",
    "        l = -jax.nn.relu(-t * self.C), 0.\n",
    "        u =  jax.nn.relu( t * self.C), 0.\n",
    "        \n",
    "        # formulate and solve quadratic programming problem\n",
    "        hyper_params = dict(params_obj=(X, -t), params_eq=None, params_ineq=(l, u))\n",
    "        osqp = BoxOSQP(matvec_Q=matvec_Q, matvec_A=matvec_A, tol=self.tolerance)\n",
    "        params, _ = osqp.run(init_params=None, **hyper_params)\n",
    "        beta = params.primal[0]\n",
    "\n",
    "        # for support vector indices: if true, then the corresponding sample is a support vector\n",
    "        is_sv = self.get_support_vectors(beta)\n",
    "        \n",
    "        return beta, is_sv\n",
    "\n",
    "    def get_support_vectors(self, beta):\n",
    "        # this sucks in JAX because it is not jittable due to boolean indexing\n",
    "        # beta is signed \n",
    "        # beta = 0 means the Langrange multiplier is 0, which means the corresponding sample does not contribute to the sum in the objective function.\n",
    "        # beta ~= 0 means the samples are support vectors\n",
    "        \n",
    "        is_sv = jnp.abs(beta) > self.tolerance\n",
    "\n",
    "        # have to return True/False array instead of indices for True. The latter is not jittable\n",
    "        #res = jnp.where(is_sc)\n",
    "        return is_sv\n",
    "    \n",
    "    def _accuracy(self):\n",
    "        \"\"\"get accuracy of model:\n",
    "            if 0 < abs(beta) < C, then epsilon = 0, then the sample is on the margin\n",
    "            if abs(beta) = C, then the sample can lie inside the margin and can either be correctly classied if epsilon <=1 or misclassified if epsilon > 1\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X_test, X_train, y_train, beta, sv_index):\n",
    "        \n",
    "        \"\"\"solving primal problem gives w and b\n",
    "            From Eq. (7.29) and (7.37) in Bishop's book:\n",
    "            $$ w = \\sum_{i=1}^{N} \\alpha_i t_i x_i = (\\beta^T x)^T = x^T \\beta $$\n",
    "            $$ wx = w^Tx^T = \\beta^T x x^T = \\beta^T K$$\n",
    "            \n",
    "        \"\"\"\n",
    "        # get wx\n",
    "        Gram = self.kernel.kernel(self.kernel_params, X_train[sv_index], X_test)\n",
    "        wx = beta[sv_index].T @ Gram \n",
    "        \n",
    "        # get b\n",
    "        # get indice of support vectors on the margin: 0 < abs(beta) < C\n",
    "        M_mask = jnp.abs(beta[sv_index]) < self.C-self.tolerance\n",
    "        Gram_S = self.kernel.kernel(self.kernel_params, X_train[sv_index], X_train[sv_index]) # (S,S)\n",
    "\n",
    "        # define some jittable functions\n",
    "        def set_nonmargin_to_zero(x, M):\n",
    "            return jnp.where(M, x, 0)\n",
    "        \n",
    "        def get_nonzero_mean(x):\n",
    "            return jnp.mean(x, where = x != 0)\n",
    "                    \n",
    "        bv = set_nonmargin_to_zero(y_train[sv_index] - Gram_S @ beta[sv_index], M_mask)\n",
    "        b = get_nonzero_mean(bv)\n",
    "\n",
    "        # This version is not jittable, and seems slightly different for the final b\n",
    "        # b1 \n",
    "        #Gram_M = kernel.kernel(kernel_params, X_train[sv][M_mask], X_train[sv]) # (M, S)\n",
    "        #bv1 = y_train[sv][M_mask] - Gram_M @ beta[sv]\n",
    "        #b1 = jnp.mean(bv1)\n",
    "        #print(bv1.shape, b1, bv1)\n",
    "        #print(b)\n",
    "        # retur signs of wx + b: 1 or -1\n",
    "        return jnp.sign(wx + b)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "lam = 0.5\n",
    "tol = 1e-06\n",
    "num_samples = 30\n",
    "num_features = 5\n",
    "verbose = False\n",
    "\n",
    "# Prepare data.\n",
    "X, y = datasets.make_classification(n_samples=num_samples, n_features=num_features,\n",
    "                                n_classes=2,\n",
    "                                random_state=0)\n",
    "X = preprocessing.Normalizer().fit_transform(X)\n",
    "y = jnp.array(y * 2. - 1)  # Transform labels from {0, 1} to {-1., 1.}.\n",
    "\n",
    "C = 1./ lam\n",
    "\n",
    "# Compare the obtained dual coefficients.\n",
    "# kernels \n",
    "linear_kernel = Linear()\n",
    "svc = BinarySVC()\n",
    "beta, sv = svc.fit(X, y)\n",
    "sv_index = jnp.where(sv)[0]\n",
    "\n",
    "# predict\n",
    "svc_predict = jax.jit(svc.predict)\n",
    "#svc_predict = svc.predict\n",
    "y_predict = svc_predict(X_test=X, \n",
    "                        X_train=X, \n",
    "                        y_train=y, \n",
    "                        beta=beta, \n",
    "                        sv_index=sv_index)\n",
    "\n",
    "print(jnp.abs(y_predict - y).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-0.4.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
