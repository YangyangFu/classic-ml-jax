{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression\n",
    "\n",
    "solve problem:\n",
    "\n",
    "$y^{(i)} = \\theta^T x^{(i)} + \\epsilon^{(i)}$\n",
    "\n",
    "# 1.1 Assumptions\n",
    "- errors ~ $N(0, \\delta^2)$, that is, $p(\\epsilon^{(i)}) = \\frac{1}{\\sqrt {2\\pi} \\delta} exp(-\\frac{(\\epsilon ^{(i)})^2}{2\\delta^2})$\n",
    "    - with rearangement, we have $p(y^{(i)}|x^{(i); \\theta}) = \\frac{1}{\\sqrt {2\\pi} \\delta} exp(-\\frac{(y^{(i)} - \\theta^Tx^{(i)})^2}{2\\delta^2})$\n",
    "- samples: independent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Likelihood Function\n",
    "\n",
    "**Likelihodd Function**:\n",
    "\n",
    "\\begin{equation}\n",
    "\n",
    "    L(\\theta) = p(y^{(1,...,m)} | x^{(1,...,m)}; \\theta) = \\Pi_{i=1}^m p(y^i|x^i, \\theta)\n",
    "    \n",
    "\\end{equation}\n",
    "\n",
    "**Notes**\n",
    "- what parameter can predict the given target when combined with given features -> purpose of maximum likelihood\n",
    "- the maximum probability that the parameter $\\theta$ can make ALL the prediction matches ALL the measurement\n",
    "- due to the independency of samples, the above equation holds.\n",
    "\n",
    "\n",
    "**Log-likelihood Function**\n",
    "\n",
    "\\begin{align}\n",
    " log L(\\theta) &= \\sum_{i=1}^{m} log (p(y^i|x^i; \\theta)) \\\\\n",
    "    &= \\sum_{i=1}^{m} log (\\frac{1}{\\sqrt {2\\pi} \\delta} exp(-\\frac{(y^i - \\theta^T(x^i)^2}{2\\delta^2})) \\\\\n",
    "    &= m log (\\frac{1}{\\sqrt{2\\pi}\\delta}) - \\frac{1}{\\delta^2} \\cdot \\frac{1}{2} \\sum_{i=0}^{m} (y^i - \\theta^Tx^i)^2 \\\\\n",
    "    &= constant - \\frac{1}{\\delta^2} \\cdot J(\\theta)\n",
    "\\end{align}\n",
    "\n",
    "where $constant$ is a constant value, and $J(\\theta)$ is typicall known as the least-sqaures cost function, or mean square errors.\n",
    "\n",
    "**Notes**\n",
    "- easy calculation by changing production to additions.\n",
    "- maximizing the log-likelihood function with respect to $\\theta$ is equivalent to minimizing the means squared error. These two have different values but the same location of the optimum.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Propeties of Maximum Likelihood \n",
    "The main appeal of the maximum likelihood estimator is that it can be shown to be the best estimator asymptotically, as the number of examples $m \\rightarrow \\infty$, in terms of its rate of convergence as `m` increases.\n",
    "\n",
    "Under appropriate conditions, the maximum likelihood estimator has the property of consistency, meaning that as the number of training examples approaches inﬁnity, the maximum likelihood estimate of a parameter\n",
    "converges to the true value of the parameter. These conditions are as follows:\n",
    "- The true distribution $p_{data}$ must lie within the model family $p_{model}(·;θ)$.\n",
    "Otherwise, no estimator can recover $p_{data}$.\n",
    "- The true distribution $p_{data}$ must be identiﬁable, meaning that there exists a unique value of θ that maximizes the likelihood function $p_{data}(·;θ)$. Otherwise, no estimator can recover $p_{data}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Implementation\n",
    "Linear regression is simply a dense neural network with a single neuron and no activation function. The neuron computes the weighted sum of its inputs and adds a bias term. The weights and bias are the parameters that the model will learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'params': {'b': (), 'w': (5, 1)}}\n",
      "output:\n",
      " [[-0.487528  ]\n",
      " [-0.46083403]\n",
      " [ 0.13756503]\n",
      " [-0.13930888]\n",
      " [-0.29863402]\n",
      " [-0.47391087]\n",
      " [-0.16525263]\n",
      " [-0.40750027]\n",
      " [-0.32280582]\n",
      " [-0.23942292]]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from flax.core import freeze, unfreeze\n",
    "import flax.linen as nn\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    features: int\n",
    "    kernel_init: nn.initializers.Initializer = nn.initializers.lecun_normal()\n",
    "    bias_init: nn.initializers.Initializer = nn.initializers.zeros\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        w = self.param('w', # name of the parameter \n",
    "                       self.kernel_init, # initialization\n",
    "                       (x.shape[-1], self.features) # shape of the parameter\n",
    "                       ) \n",
    "        b = self.param('b', self.bias_init, ())\n",
    "        return jnp.dot(x, w) + b\n",
    "\n",
    "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
    "x = random.uniform(key1, (10,5))\n",
    "\n",
    "# create a dense layer (linear model) with 5 features\n",
    "model = Linear(features=1)\n",
    "params = model.init(key2, x)\n",
    "\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(params)))\n",
    "print('output:\\n', y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-0.4.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
